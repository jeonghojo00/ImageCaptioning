{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeonghojo00/ImageCaptioning/blob/main/ImgCaptioning_Transformer_flickr8k.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITQKi60ecc7e"
      },
      "source": [
        "#0. Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9RrduYbcmcE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0941e652-4f34-41c4-b551-08e316ff744f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydMo8oIYcsPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ec126f6-5fff-48cc-9e2b-48c8d11db466"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/ImageCaptioning\n",
            "captions_val2014_fakecap_results.json  lstm_decoder.py\n",
            "captions_val2014.json\t\t       main.py\n",
            "checkpoint\t\t\t       model\n",
            "code\t\t\t\t       models\n",
            "cs7643-final-project\t\t       models.py\n",
            "data\t\t\t\t       pretraining.py\n",
            "data_loader.py\t\t\t       __pycache__\n",
            "efficientnet.py\t\t\t       resize_image.py\n",
            "eval.pkl\t\t\t       resnet.py\n",
            "experiments\t\t\t       show_attend_tell.py\n",
            "image_captioning_loss_200731_2.pkl     split_caption.py\n",
            "image_captioning_loss_200731.pkl       util\n",
            "inception.py\t\t\t       utils.py\n",
            "inf_result.pkl\t\t\t       vgg.py\n",
            "learned_model.npy\t\t       vocab.pkl\n",
            "learned_models\t\t\t       vocabulary.py\n",
            "load_data.py\n"
          ]
        }
      ],
      "source": [
        "# Change directory to the package folder\n",
        "%cd '/content/drive/MyDrive/Colab Notebooks/ImageCaptioning/'\n",
        "# Verify the contents of the current folder\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xi74KD1Y2RmM"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import subprocess\n",
        "import pkg_resources\n",
        "\n",
        "required = {'efficientnet_pytorch', 'timm', 'tqdm', 'torch', 'torchvision'}\n",
        "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
        "missing = required - installed\n",
        "\n",
        "if missing:\n",
        "    python = sys.executable\n",
        "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNiNq3zvv_bk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08a2c13f-3b11-4860-cb8f-0ca894465f87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import Libraries\n",
        "import os\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import nltk\n",
        "from collections import Counter\n",
        "import timm\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
        "import torch.optim as optim \n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from load_data import *\n",
        "from resize_image import *\n",
        "from split_caption import *"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Preparation of Images (Train/Val/Test split)"
      ],
      "metadata": {
        "id": "aY7qh7wMzOVi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess images"
      ],
      "metadata": {
        "id": "lqZswf6hGzP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data folder\n",
        "image_dir = './data/flickr8k/Images'\n",
        "caption_path = './data/flickr8k/captions.txt' # Original caption file with path\n",
        "\n",
        "train_image_dir = './data/flickr8k/train/Images' # Resized train images folder\n",
        "val_image_dir = './data/flickr8k/val/Images' # Resized validation images folder\n",
        "test_image_dir = './data/flickr8k/test/Images' # Resized test images folder\n",
        "train_caption_path = \"./data/flickr8k/train/captions.txt\" # Resized train images' captions\n",
        "val_caption_path = \"./data/flickr8k/val/captions.txt\" # Resized validation images' captions\n",
        "test_caption_path = \"./data/flickr8k/test/captions.txt\" # Resized test images' captions\n",
        "num_train_images = 6000\n",
        "num_val_images = 1000\n",
        "\n",
        "\n",
        "vocab_path = \"./vocab.pkl\" # vocabulary file\n",
        "word_threshold = 4 # Minimum occurrances of words\n",
        "\n",
        "resizeImage_required = False\n",
        "if resizeImage_required == True:\n",
        "    save_resized_images(image_dir, train_image_dir, val_image_dir, test_image_dir, num_train_images, num_val_images, resize_size=None)"
      ],
      "metadata": {
        "id": "z3ZAHMJEswuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkBSBKWKDtdv"
      },
      "source": [
        "## Preprocess captions for vocab dictionary and caption divisions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v49kh8ncb-Ow"
      },
      "outputs": [],
      "source": [
        "splitCaption_required = False\n",
        "\n",
        "if splitCaption_required == True:\n",
        "    split_caption(caption_path, train_caption_path, val_caption_path, test_caption_path, vocab_path, num_train_images, num_val_images, word_threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybg10OqBRhCv",
        "outputId": "cfa183f1-b8a6-41ff-a51c-e2ee7fb3557d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30000 ./data/flickr8k/train/captions.txt\n",
            "5000 ./data/flickr8k/val/captions.txt\n",
            "5455 ./data/flickr8k/test/captions.txt\n"
          ]
        }
      ],
      "source": [
        "# Number of Traning data\n",
        "!wc -l ./data/flickr8k/train/captions.txt\n",
        "# Number of Validation data\n",
        "!wc -l ./data/flickr8k/val/captions.txt\n",
        "# Number of Testing data\n",
        "!wc -l ./data/flickr8k/test/captions.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Build Model"
      ],
      "metadata": {
        "id": "-Rl3Y3IKrReT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Patch Embedding"
      ],
      "metadata": {
        "id": "G5o2mQRxA3fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\"Split images into patches and embed them\n",
        "\n",
        "    Paramters\n",
        "    ---------\n",
        "    img_size : int\n",
        "        Input size(Height or Width) of an image (it is square)\n",
        "    \n",
        "    patch_size : int\n",
        "        Patch size to be splitted\n",
        "    \n",
        "    in_chans : int\n",
        "        Number of channels (1 or gray scale and 3 for RGB)\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    num_patches : int\n",
        "        Number of patches per image\n",
        "    \n",
        "    embed_dim : int\n",
        "        Feature dimension\n",
        "\n",
        "    patches : nn.Conv2d\n",
        "        Convolution layer that splits an images into patches and embedding them\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size, patch_size = 16, in_chans = 3, embed_dim = 768):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.in_chans = in_chans\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.n_patches = (img_size//patch_size)**2\n",
        "        \n",
        "        self.patching = nn.Conv2d(\n",
        "            in_chans,\n",
        "            embed_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride=patch_size\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        \"\"\"Run forward pass\n",
        "\n",
        "        Parmaters\n",
        "        ---------\n",
        "        x : torch.Tensor\n",
        "            Shape: (n_samples, in_chans, img_size, img_size)\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "\n",
        "            Shape : (n_samples, num_patches, embed_dim)\n",
        "        \"\"\"\n",
        "        out = self.patching(x) # (n_samples, embed_dim, H/patch_size, W/patch_size)\n",
        "        out = out.flatten(2)   # (n_samples, embed_dim, num_patches)\n",
        "        out = out.permute(0, 2, 1) # (n_samples, num_patches, embed_dim)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "to-ILj19gVoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of patching\n",
        "imgs = torch.ones([3, 3, 384, 384])\n",
        "img_size = imgs.shape[2]\n",
        "patch_size = 16\n",
        "in_chans = imgs.shape[1]\n",
        "embed_dim = (patch_size**2) * in_chans\n",
        "patching = PatchEmbed(img_size = img_size, patch_size = patch_size, in_chans = in_chans, embed_dim = embed_dim)\n",
        "\n",
        "print(\"Input shape: \", imgs.shape)\n",
        "afterPatch = patching(imgs)\n",
        "print(\"Shapes after Patching: \", afterPatch.shape)\n",
        "print(f\"{afterPatch.shape[0]} : image channels, \\n{afterPatch.shape[1]} : number of patches, \\n{afterPatch.shape[2]} : embedding dimension\")"
      ],
      "metadata": {
        "id": "cSm7Yryz5UEF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14089f0b-dbb3-49b0-f833-4b27b6b5eeac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape:  torch.Size([3, 3, 384, 384])\n",
            "Shapes after Patching:  torch.Size([3, 576, 768])\n",
            "3 : image channels, \n",
            "576 : number of patches, \n",
            "768 : embedding dimension\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2_2. Transformer (Custom)"
      ],
      "metadata": {
        "id": "Ry6Pk1kDA7LH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
        "    \n",
        "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        mask = mask.unsqueeze(1)\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    \n",
        "    scores = F.softmax(scores, dim=-1)\n",
        "    \n",
        "    if dropout is not None:\n",
        "        scores = dropout(scores)\n",
        "        \n",
        "    output = torch.matmul(scores, v)\n",
        "    return output\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_heads, embed_dim, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_model = embed_dim\n",
        "        self.d_k = embed_dim // n_heads\n",
        "        self.h = n_heads\n",
        "        \n",
        "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.out = nn.Linear(embed_dim, embed_dim)\n",
        "    \n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        \n",
        "        bs = q.size(0)\n",
        "        \n",
        "        # perform linear operation and split into h heads\n",
        "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
        "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
        "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
        "        \n",
        "        # transpose to get dimensions bs * h * sl * d_model\n",
        "        k = k.transpose(1,2)\n",
        "        q = q.transpose(1,2)\n",
        "        v = v.transpose(1,2)\n",
        "        # calculate attention using function we will define next\n",
        "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
        "        \n",
        "        # concatenate heads and put through final linear layer\n",
        "        concat = scores.transpose(1,2).contiguous()\\\n",
        "        .view(bs, -1, self.d_model)\n",
        "        \n",
        "        output = self.out(concat)\n",
        "    \n",
        "        return output\n",
        "\n",
        "\n",
        "class FFN(nn.Module):\n",
        "    \"\"\"Feed Forward Network (Multilayer perceptron)\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    in_features : int\n",
        "        Number of input features\n",
        "    \n",
        "    hidden_features : int\n",
        "        Number of nodes in the hidden layer\n",
        "\n",
        "    out_features : int\n",
        "        Number of output features\n",
        "\n",
        "    dropout : float\n",
        "        Dropout proability\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    fc1 : nn.Linear\n",
        "        The first linear layer\n",
        "\n",
        "    fc2 : nn.Linear\n",
        "        The second linear layer\n",
        "\n",
        "    drop : nn.Dropout\n",
        "        Dropout layer\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, hidden_features, out_features, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"Run forward pass\n",
        "        FFN(x) = FC2(Dropout(GELU(FC1(x))))\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Shape : (n_samples, n_patches, in_features)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Shape : (n_samples, n_patches, out_features)\n",
        "        \"\"\"\n",
        "        out = self.drop(F.gelu(self.fc1(x))) # (n_samples, n_patches, hidden_features) \n",
        "        out = self.fc2(out)  # (n_samples, n_patches, out_features) \n",
        "        return out\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    \"\"\"Transformer Encoder block\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    embed_dim : int\n",
        "        Embedding dimension\n",
        "\n",
        "    n_heads : int\n",
        "        Number of attention heads\n",
        "    \n",
        "    mlp_ratio : float\n",
        "        Determines the hidden dimension size of the MLP module with respect to dim\n",
        "\n",
        "    attn_p, ffn_p : float\n",
        "        Dropout probability\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    norm1, norm2 = LayerNorm\n",
        "        Layer normalization for each block\n",
        "\n",
        "    attn : Attention\n",
        "        Attention Module\n",
        "\n",
        "    ffn : FFN\n",
        "        FFN Module\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, n_heads, mlp_ratio = 4.0, attn_p=0.0, ffn_p=0.0):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim, eps = 1e-6)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim, eps = 1e-6)\n",
        "        self.attn = MultiHeadAttention(n_heads = n_heads, \n",
        "                                       embed_dim = embed_dim,\n",
        "                                       dropout=attn_p)\n",
        "    \n",
        "        hidden_features = int(embed_dim * mlp_ratio)\n",
        "        self.ffn = FFN(embed_dim, \n",
        "                       hidden_features = hidden_features, \n",
        "                       out_features = embed_dim,\n",
        "                       dropout=ffn_p)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attention = self.attn(x, x, x, mask)\n",
        "        out = self.norm1(attention+x)\n",
        "\n",
        "        forward = self.ffn(out)\n",
        "        out = self.norm2(forward + out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Positional Encoding\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    embed_dim : int\n",
        "        Embedding dimension\n",
        "    \n",
        "    drouput : float\n",
        "        Dropout probability\n",
        "    \n",
        "    max_len : int\n",
        "        Max length of the input\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, dropout = 0.1, max_len = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * (-math.log(10000.0) / embed_dim))\n",
        "        pe = torch.zeros(max_len, 1, embed_dim)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Run the forward pass\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class Custom_TransformerEncoder(nn.Module):\n",
        "    \"\"\"Customized Transformer Encoder\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    img_size : int\n",
        "        Both height and the width of the image (H and W of a square)\n",
        "    \n",
        "    patch_size : int\n",
        "        Both height and the width of the patch (h and w of a square)\n",
        "\n",
        "    in_chans : int\n",
        "        Number of input channels (1 for gray scale and 3 for RGB)\n",
        "\n",
        "    embed_dim : int\n",
        "        Dimensionality of the token embeddings\n",
        "    \n",
        "    depth : int\n",
        "        Number of blocks\n",
        "\n",
        "    n_heads : int\n",
        "        Number of attention heads\n",
        "\n",
        "    mlp_ratio : float\n",
        "        Determines the hidden features of the \"MLP\" module\n",
        "\n",
        "    pos_p, attn_p, ffn_p: float\n",
        "        Dropout probability\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    patch_embed : PatchEmbed\n",
        "        Instance of \"PatchEmbed\" layer\n",
        "    \n",
        "    pos_encoder : Positional Encoder\n",
        "        Instance of \"PositionalEncoder\" layer\n",
        "\n",
        "    blocks: nn.ModuleList\n",
        "        List of \"Block\" modules\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 img_size = 384,\n",
        "                 patch_size = 16,\n",
        "                 in_chans = 3,\n",
        "                 embed_dim = 768,\n",
        "                 depth = 12,\n",
        "                 n_heads = 12,\n",
        "                 mlp_ratio = 4.,\n",
        "                 pos_p = 0.,\n",
        "                 attn_p = 0.,\n",
        "                 ffn_p = 0.\n",
        "                 ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_embed = PatchEmbed(\n",
        "            img_size = img_size,\n",
        "            patch_size = patch_size,\n",
        "            in_chans = in_chans,\n",
        "            embed_dim = embed_dim,\n",
        "        )\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding(embed_dim = embed_dim,\n",
        "                                              dropout = pos_p,\n",
        "                                              max_len = self.patch_embed.n_patches)\n",
        "\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [\n",
        "                EncoderBlock (\n",
        "                    embed_dim = embed_dim,\n",
        "                    n_heads = n_heads,\n",
        "                    mlp_ratio = mlp_ratio,\n",
        "                    attn_p = attn_p,\n",
        "                    ffn_p = ffn_p,\n",
        "                )\n",
        "                for _ in range(depth)\n",
        "            ] \n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"Run the forward pass.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Shape : (n_samples, in_chans, img_size, img_size)\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        logits : torch.Tensor\n",
        "            Logits over all the classes : (n_samples, n_classes)\n",
        "        \"\"\"\n",
        "        # Patch embedding\n",
        "        x = self.patch_embed(x)\n",
        "        # Add Positional Embedding\n",
        "        x = self.pos_encoder(x) # (n_samples, n_patches, embed_dim)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x, mask)\n",
        "            \n",
        "        return x"
      ],
      "metadata": {
        "id": "mGYqeyYr-MFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"Transformer Decoder block\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    embed_dim : int\n",
        "        Embedding dimension\n",
        "\n",
        "    n_heads : int\n",
        "        Number of attention heads\n",
        "    \n",
        "    mlp_ratio : float\n",
        "        Determines the hidden dimension size of the MLP module with respect to dim\n",
        "\n",
        "    attn_p, ffn_p : float\n",
        "        Dropout probability\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    norm1, norm2, norm3 = LayerNorm\n",
        "        Layer normalization for each sublayer\n",
        "\n",
        "    mask_attn = Masked Self-Attention\n",
        "        Masked Self-Attention\n",
        "\n",
        "    cross_attn : Cross Attention\n",
        "        Cross Attention Module with input of output of encoder.\n",
        "        Memory key and value come from the output of the encoder.\n",
        "\n",
        "    ffn : FFN\n",
        "        FFN Module\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, n_heads, mlp_ratio = 4.0, attn_p=0.0, ffn_p=0.0):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim, eps = 1e-6)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim, eps = 1e-6)\n",
        "        self.norm3 = nn.LayerNorm(embed_dim, eps = 1e-6)\n",
        "        self.mask_attn = MultiHeadAttention(n_heads = n_heads, \n",
        "                                       embed_dim = embed_dim,\n",
        "                                       dropout=attn_p)\n",
        "        self.cross_attn = MultiHeadAttention(n_heads = n_heads, \n",
        "                                       embed_dim = embed_dim,\n",
        "                                       dropout=attn_p)\n",
        "    \n",
        "        hidden_features = int(embed_dim * mlp_ratio)\n",
        "        self.ffn = FFN(embed_dim, \n",
        "                       hidden_features = hidden_features, \n",
        "                       out_features = embed_dim,\n",
        "                       dropout=ffn_p)\n",
        "\n",
        "    def forward(self, x, memory, src_mask=None, trg_mask=None):\n",
        "        self_attn = self.mask_attn(x, x, x, trg_mask)\n",
        "        out = self.norm1(self_attn + x)\n",
        "\n",
        "        cross_attn = self.cross_attn(out, memory, memory, src_mask)\n",
        "        out = self.norm2(cross_attn + out)\n",
        "\n",
        "        forward = self.ffn(out)\n",
        "        out = self.norm3(forward + out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Custom_TransformerDecoder(nn.Module):\n",
        "    \"\"\"Customized Transformer Decoder\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    img_size : int\n",
        "        Both height and the width of the image (H and W of a square)\n",
        "    \n",
        "    patch_size : int\n",
        "        Both height and the width of the patch (h and w of a square)\n",
        "\n",
        "    in_chans : int\n",
        "        Number of input channels (1 for gray scale and 3 for RGB)\n",
        "\n",
        "    embed_dim : int\n",
        "        Dimensionality of the token embeddings\n",
        "    \n",
        "    depth : int\n",
        "        Number of blocks\n",
        "\n",
        "    n_heads : int\n",
        "        Number of attention heads\n",
        "\n",
        "    mlp_ratio : float\n",
        "        Determines the hidden features of the \"MLP\" module\n",
        "\n",
        "    pos_p, attn_p, ffn_p: float\n",
        "        Dropout probability\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    word_embed : PatchEmbed\n",
        "        Word embedding\n",
        "    \n",
        "    pos_encoder : Positional Encoder\n",
        "        Instance of \"PositionalEncoder\" layer\n",
        "\n",
        "    blocks: nn.ModuleList\n",
        "        List of \"Block\" modules\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 vocab_size = 336037,\n",
        "                 embed_dim = 768,\n",
        "                 depth = 12,\n",
        "                 n_heads = 12,\n",
        "                 mlp_ratio = 4.,\n",
        "                 pos_p = 0.,\n",
        "                 attn_p = 0.,\n",
        "                 ffn_p = 0.\n",
        "                 ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.word_embed = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        self.pos_decoder = PositionalEncoding(embed_dim = embed_dim,\n",
        "                                              dropout = pos_p,\n",
        "                                              max_len = vocab_size)\n",
        "\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [\n",
        "                DecoderBlock (\n",
        "                    embed_dim = embed_dim,\n",
        "                    n_heads = n_heads,\n",
        "                    mlp_ratio = mlp_ratio,\n",
        "                    attn_p = attn_p,\n",
        "                    ffn_p = ffn_p,\n",
        "                )\n",
        "                for _ in range(depth)\n",
        "            ] \n",
        "        )\n",
        "\n",
        "    def forward(self, x, memory, src_mask=None, trg_mask=None):\n",
        "        \"\"\"Run the forward pass.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Shape : (n_samples, in_chans, img_size, img_size)\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        logits : torch.Tensor\n",
        "            Logits over all the classes : (n_samples, n_classes)\n",
        "        \"\"\"\n",
        "        # Word embedding\n",
        "        x = self.word_embed(x)\n",
        "        # Add Positional Embedding\n",
        "        x = self.pos_decoder(x) # (n_samples, n_patches, embed_dim)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x, memory, src_mask, trg_mask)\n",
        "            \n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ga5HaCyiZU4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgs, captions = next(iter(train_loader))\n",
        "print(imgs.shape)\n",
        "print(captions.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOPMIVgdkLSz",
        "outputId": "2e53f4b1-aa67-4149-b264-e715e3e9128d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 3, 384, 384])\n",
            "torch.Size([10, 18])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "\n",
        "# Example of Transformer\n",
        "img_size = 384\n",
        "patch_size = 16\n",
        "in_chans = imgs.shape[1]\n",
        "mlp_ratio = 4.\n",
        "embed_dim = 256#(patch_size**2) * in_chans\n",
        "hidden_features = int(embed_dim * mlp_ratio)\n",
        "encoder_depth=12\n",
        "encoder_heads=8\n",
        "decoder_depth=4\n",
        "decoder_heads=8\n",
        "pos_p=0.0\n",
        "attn_p=0.1\n",
        "ffn_p=0.1\n",
        "vocab_size=5000\n",
        "assert embed_dim % encoder_heads == 0, \"embed_dim must be a multiple of encoder_heads\"\n",
        "assert embed_dim % decoder_heads == 0, \"embed_dim must be a multiple of decoder_heads\"\n",
        "\n",
        "tr_enc = Custom_TransformerEncoder(img_size = img_size,\n",
        "                                   patch_size = patch_size,\n",
        "                                   in_chans = in_chans,\n",
        "                                   embed_dim = embed_dim,\n",
        "                                   depth = encoder_depth,\n",
        "                                   n_heads = encoder_heads,\n",
        "                                   mlp_ratio = mlp_ratio,\n",
        "                                   pos_p = pos_p,\n",
        "                                   attn_p = attn_p,\n",
        "                                   ffn_p = ffn_p\n",
        "                                   )\n",
        "\n",
        "tr_dec = Custom_TransformerDecoder(vocab_size = vocab_size,\n",
        "                                    embed_dim = embed_dim,\n",
        "                                    depth = decoder_depth,\n",
        "                                    n_heads = decoder_heads,\n",
        "                                    mlp_ratio = mlp_ratio,\n",
        "                                    pos_p = pos_p,\n",
        "                                    attn_p = attn_p,\n",
        "                                    ffn_p = ffn_p\n",
        "                                    )\n",
        "\n",
        "memory = tr_enc(imgs)"
      ],
      "metadata": {
        "id": "lKuocqu8ujZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def create_tgt_mask(tgt):\n",
        "    target_seq = tgt\n",
        "    target_pad = train_dataset.vocab.stoi['<PAD>']\n",
        "    target_msk = (target_seq != target_pad).unsqueeze(1)\n",
        "    size = target_seq.size(1) # get seq_len for matrix\n",
        "\n",
        "    nopeak_mask = np.triu(np.ones((1,size,size), dtype=int), k=1).astype('uint8')\n",
        "    nopeak_mask = Variable(torch.from_numpy(nopeak_mask) == 0)\n",
        "    target_msk = target_msk & nopeak_mask\n",
        "    return target_msk"
      ],
      "metadata": {
        "id": "-kqwgnfEp3vW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
        "y_input = captions[:,:-1]\n",
        "y_expected = captions[:,1:]\n",
        "\n",
        "tgt_mask = create_tgt_mask(y_input)\n",
        "# Forward\n",
        "\n",
        "output = tr_dec(y_input, memory, trg_mask=target_msk)"
      ],
      "metadata": {
        "id": "LSoqxuIHlES5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Custom_Transformer(nn.Module):\n",
        "    def __init__(self, img_size=384, patch_size=16, in_chans=3, embed_dim=768, vocab_size=336037, encoder_depth=12, decoder_depth=12, encoder_heads=12, decoder_heads=12, mlp_ratio=4.0, pos_p=0.0, attn_p=0.1, ffn_p=0.1):\n",
        "        super().__init__()\n",
        "        assert embed_dim % encoder_heads == 0 and embed_dim % decoder_heads == 0, \"Embed dim must be a multiple of encoder heads and decoder heads\"\n",
        "        self.encoder = Custom_TransformerEncoder(img_size = img_size,\n",
        "                                                 patch_size = patch_size,\n",
        "                                                 in_chans = in_chans,\n",
        "                                                 embed_dim = embed_dim,\n",
        "                                                 depth = encoder_depth,\n",
        "                                                 n_heads = encoder_heads,\n",
        "                                                 mlp_ratio = mlp_ratio,\n",
        "                                                 pos_p = pos_p,\n",
        "                                                 attn_p = attn_p,\n",
        "                                                 ffn_p = ffn_p\n",
        "                                                 )\n",
        "        self.decoder = Custom_TransformerDecoder(vocab_size = vocab_size,\n",
        "                                                 embed_dim = embed_dim,\n",
        "                                                 depth = decoder_depth,\n",
        "                                                 n_heads = decoder_heads,\n",
        "                                                 mlp_ratio = mlp_ratio,\n",
        "                                                 pos_p = pos_p,\n",
        "                                                 attn_p = attn_p,\n",
        "                                                 ffn_p = ffn_p\n",
        "                                                 )\n",
        "        self.final_layer = nn.Linear(embed_dim, vocab_size)\n",
        "        \n",
        "    def forward(self, src, trg, src_mask=None, trg_mask=None):\n",
        "        memory = self.encoder(src, src_mask)\n",
        "        output = self.decoder(trg, memory, src_mask, trg_mask)\n",
        "        output = self.final_layer(output)\n",
        "        return output\n",
        "        # we don't perform softmax on the output as this will be handled \n",
        "        # automatically by our loss function\n",
        "\n",
        "    def create_tgt_mask(self, tgt):\n",
        "        target_seq = tgt\n",
        "        target_pad = 0\n",
        "        target_msk = (target_seq != target_pad).unsqueeze(1)\n",
        "        size = target_seq.size(1) # get seq_len for matrix\n",
        "\n",
        "        nopeak_mask = np.triu(np.ones((1,size,size), dtype=int), k=1).astype('uint8')\n",
        "        nopeak_mask = Variable(torch.from_numpy(nopeak_mask) == 0)\n",
        "        target_msk = target_msk & nopeak_mask\n",
        "        return target_msk"
      ],
      "metadata": {
        "id": "9CjJWbREYszg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2_3. Practice building model"
      ],
      "metadata": {
        "id": "HANXhSr_raT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config(object):\n",
        "    def __init__(self):\n",
        "\n",
        "        # Learning Rates\n",
        "        self.lr_backbone = 1e-5\n",
        "        self.lr = 1e-4\n",
        "\n",
        "        # Epochs\n",
        "        self.epochs = 30\n",
        "        self.lr_drop = 20\n",
        "        self.start_epoch = 0\n",
        "        self.weight_decay = 1e-4\n",
        "\n",
        "        # Backbone\n",
        "        self.backbone = 'resnet101'\n",
        "        self.position_embedding = 'sine'\n",
        "        self.dilation = True\n",
        "        \n",
        "        # Basic\n",
        "        self.device = 'cuda'\n",
        "        self.seed = 42\n",
        "        self.batch_size = 32\n",
        "        self.num_workers = 8\n",
        "        self.checkpoint = './checkpoint.pth'\n",
        "        self.clip_max_norm = 0.1\n",
        "\n",
        "        # Transformer\n",
        "        self.hidden_dim = 256\n",
        "        self.pad_token_id = 0\n",
        "        self.max_position_embeddings = 128\n",
        "        self.layer_norm_eps = 1e-12\n",
        "        self.dropout = 0.1\n",
        "        self.vocab_size = 30522\n",
        "\n",
        "        self.enc_layers = 6\n",
        "        self.dec_layers = 6\n",
        "        self.dim_feedforward = 2048\n",
        "        self.nheads = 8\n",
        "        self.pre_norm = True\n",
        "\n",
        "        # Dataset\n",
        "        self.dir = '../coco'\n",
        "        self.limit = -1"
      ],
      "metadata": {
        "id": "x8oV2YLodMa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config()\n",
        "\n",
        "class DecoderEmbeddings(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding(\n",
        "            config.vocab_size, config.hidden_dim, padding_idx=config.pad_token_id)\n",
        "        self.position_embeddings = nn.Embedding(\n",
        "            config.max_position_embeddings, config.hidden_dim\n",
        "        )\n",
        "\n",
        "        self.LayerNorm = torch.nn.LayerNorm(\n",
        "            config.hidden_dim, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        input_shape = x.size()\n",
        "        seq_length = input_shape[1]\n",
        "        device = x.device\n",
        "\n",
        "        position_ids = torch.arange(\n",
        "            seq_length, dtype=torch.long, device=device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand(input_shape)\n",
        "\n",
        "        input_embeds = self.word_embeddings(x)\n",
        "        position_embeds = self.position_embeddings(position_ids)\n",
        "\n",
        "        embeddings = input_embeds + position_embeds\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "s-9j1WPGYnLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\" Very simple multi-layer perceptron (also called FFN)\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        h = [hidden_dim] * (num_layers - 1)\n",
        "        self.layers = nn.ModuleList(nn.Linear(n, k)\n",
        "                                    for n, k in zip([input_dim] + h, h + [output_dim]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
        "        return x\n",
        "\n",
        "mlp = MLP(256, 512, 30522, 3)\n",
        "mlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seC8rFqD98a0",
        "outputId": "923dc516-34e4-48c4-d1bd-1dffc1625e71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (layers): ModuleList(\n",
              "    (0): Linear(in_features=256, out_features=512, bias=True)\n",
              "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (2): Linear(in_features=512, out_features=30522, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKHVQUYlc7ZP"
      },
      "source": [
        "#3. Prepare dataset and dataloader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import spacy\n",
        "\n",
        "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "class Vocabulary(object):\n",
        "    def __init__(self, freq_threshold):\n",
        "        self.stoi = {}\n",
        "        self.itos = {}\n",
        "        self.idx = 0\n",
        "        self.freq_threshold = freq_threshold\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if not word in self.stoi:\n",
        "            self.stoi[word] = self.idx\n",
        "            self.itos[self.idx] = word\n",
        "            self.idx += 1\n",
        "\n",
        "    def __call__(self, word):\n",
        "        if not word in self.stoi:\n",
        "            return self.stoi['<UNK>']\n",
        "        return self.stoi[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.stoi)\n",
        "    \n",
        "    @staticmethod\n",
        "    def tokenizer_eng(text):\n",
        "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "    def build_vocabulary(self, sentences):\n",
        "        counter = Counter()\n",
        "        for caption in sentences:\n",
        "            caption = caption.translate(str.maketrans('', '', string.punctuation)).strip()\n",
        "            tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
        "            counter.update(tokens) # 각 토큰의 개수 세기\n",
        "\n",
        "        words = [word for word, cnt in counter.items() if cnt >= self.freq_threshold]\n",
        "\n",
        "        self.add_word('<PAD>')\n",
        "        self.add_word('<SOS>')\n",
        "        self.add_word('<EOS>')\n",
        "        self.add_word('<UNK>') # unknown 토큰\n",
        "\n",
        "        # Vocabulary 객체에 모든 단어를 담기\n",
        "        for word in words:\n",
        "            self.add_word(word)\n",
        "    \n",
        "    def numericalize(self,sentence):\n",
        "        tokenized_text = self.tokenizer_eng(sentence)\n",
        "        \n",
        "        return [self.stoi[word] if word in self.stoi else self.stoi[\"<UNK>\"] for word in tokenized_text ]\n",
        "                    \n",
        "class FlickrDataset(Dataset):\n",
        "    def __init__(self, root_dir='./data/flickr8k/Images', caption_path='./data/flickr8k/captions.txt', freq_threshold=10, transform=None):\n",
        "        self.freq_threshold = freq_threshold\n",
        "        self.transform = transform\n",
        "        self.root_dir = root_dir\n",
        "    \n",
        "        self.df = pd.read_csv(caption_path, names=['image', 'caption'])\n",
        "        \n",
        "        self.captions = self.df['caption']\n",
        "        self.images = self.df['image']\n",
        "        \n",
        "        self.vocab = Vocabulary(freq_threshold)\n",
        "        \n",
        "        print(len(self.captions.tolist()))\n",
        "        self.vocab.build_vocabulary(self.captions.tolist())\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        caption = self.captions[index]\n",
        "        image = self.images[index]\n",
        "        \n",
        "        img = Image.open(os.path.join(self.root_dir,image)).convert(\"RGB\")\n",
        "        \n",
        "        if (self.transform):\n",
        "            img = self.transform(img)\n",
        "        \n",
        "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
        "        caption = caption.translate(str.maketrans('', '', string.punctuation)).strip()\n",
        "        numericalized_caption += self.vocab.numericalize(caption)\n",
        "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
        "        \n",
        "        return img, torch.tensor(numericalized_caption)\n",
        "\n",
        "class MyCollate:\n",
        "    def __init__(self, pad_value):\n",
        "        self.pad_value = pad_value\n",
        "    \n",
        "    def __call__(self,batch):\n",
        "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
        "        img = torch.cat(imgs, dim=0)\n",
        "        targets = [item[1] for item in batch]\n",
        "        targets = pad_sequence(targets, batch_first=True, padding_value=self.pad_value)\n",
        "        \n",
        "        return img, targets\n",
        "\n",
        "\n",
        "def getLoader(root_dir=\"./data/flickr8k/train/Images\", caption_path=\"./data/flickr8k/train/captions.txt\", word_frequency=5, transform=None, batch_size=32, num_workers=0, shuffle=True, pin_memory=True):\n",
        "    dataset = FlickrDataset(root_dir=root_dir,caption_path=caption_path, freq_threshold=word_frequency, transform=transform)\n",
        "    pad_value = dataset.vocab.stoi[\"<PAD>\"]\n",
        "    \n",
        "    loader = DataLoader(dataset=dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=True, collate_fn=MyCollate(pad_value))\n",
        "    \n",
        "    return loader, dataset     "
      ],
      "metadata": {
        "id": "dBEgXYmEueUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Include caplen\n",
        "import string\n",
        "import spacy\n",
        "\n",
        "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "class Vocabulary(object):\n",
        "    def __init__(self, freq_threshold):\n",
        "        self.stoi = {}\n",
        "        self.itos = {}\n",
        "        self.idx = 0\n",
        "        self.freq_threshold = freq_threshold\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if not word in self.stoi:\n",
        "            self.stoi[word] = self.idx\n",
        "            self.itos[self.idx] = word\n",
        "            self.idx += 1\n",
        "\n",
        "    def __call__(self, word):\n",
        "        if not word in self.stoi:\n",
        "            return self.stoi['<UNK>']\n",
        "        return self.stoi[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.stoi)\n",
        "    \n",
        "    @staticmethod\n",
        "    def tokenizer_eng(text):\n",
        "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "    def build_vocabulary(self, sentences):\n",
        "        counter = Counter()\n",
        "        for caption in sentences:\n",
        "            caption = caption.translate(str.maketrans('', '', string.punctuation)).strip()\n",
        "            tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
        "            counter.update(tokens) # 각 토큰의 개수 세기\n",
        "\n",
        "        words = [word for word, cnt in counter.items() if cnt >= self.freq_threshold]\n",
        "\n",
        "        self.add_word('<PAD>')\n",
        "        self.add_word('<SOS>')\n",
        "        self.add_word('<EOS>')\n",
        "        self.add_word('<UNK>') # unknown 토큰\n",
        "\n",
        "        # Vocabulary 객체에 모든 단어를 담기\n",
        "        for word in words:\n",
        "            self.add_word(word)\n",
        "    \n",
        "    def numericalize(self,sentence):\n",
        "        tokenized_text = self.tokenizer_eng(sentence)\n",
        "        \n",
        "        return [self.stoi[word] if word in self.stoi else self.stoi[\"<UNK>\"] for word in tokenized_text ]\n",
        "                    \n",
        "class FlickrDataset(Dataset):\n",
        "    def __init__(self, root_dir='./data/flickr8k/Images', caption_path='./data/flickr8k/captions.txt', freq_threshold=10, max_cap_len = 50, transform=None):\n",
        "        self.freq_threshold = freq_threshold\n",
        "        self.transform = transform\n",
        "        self.root_dir = root_dir\n",
        "    \n",
        "        self.df = pd.read_csv(caption_path, names=['image', 'caption'])\n",
        "        \n",
        "        self.captions = self.df['caption']\n",
        "        self.images = self.df['image']\n",
        "        self.max_cap_len = max_cap_len\n",
        "        \n",
        "        self.vocab = Vocabulary(freq_threshold)\n",
        "        \n",
        "        print(len(self.captions.tolist()))\n",
        "        self.vocab.build_vocabulary(self.captions.tolist())\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        caption = self.captions[index]\n",
        "        image = self.images[index]\n",
        "        \n",
        "        img = Image.open(os.path.join(self.root_dir,image)).convert(\"RGB\")\n",
        "        \n",
        "        if (self.transform):\n",
        "            img = self.transform(img)\n",
        "        \n",
        "        '''\n",
        "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
        "        caption = caption.translate(str.maketrans('', '', string.punctuation)).strip()\n",
        "        numericalized_caption += self.vocab.numericalize(caption)\n",
        "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
        "        '''\n",
        "        c = self.vocab.tokenizer_eng(caption.translate(str.maketrans('', '', string.punctuation)).strip())\n",
        "        numericalized_caption = [self.vocab.stoi['<SOS>']] + \\\n",
        "                                [self.vocab.stoi.get(word, self.vocab.stoi['<UNK>']) for word in c] + \\\n",
        "                                [self.vocab.stoi['<EOS>']] + \\\n",
        "                                [self.vocab.stoi['<PAD>']] * (self.max_cap_len - len(c))\n",
        "        caplen = len(c)+2\n",
        "        \n",
        "        return img, torch.LongTensor(numericalized_caption), [caplen]\n",
        "'''\n",
        "class MyCollate:\n",
        "    def __init__(self, pad_value):\n",
        "        self.pad_value = pad_value\n",
        "    \n",
        "    def __call__(self,batch):\n",
        "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
        "        img = torch.cat(imgs, dim=0)\n",
        "        targets = [item[1] for item in batch]\n",
        "        caplens = [item[2] for item in batch]\n",
        "        #targets = pad_sequence(targets, batch_first=True, padding_value=self.pad_value)\n",
        "        \n",
        "        return img, targets, torch.LongTensor(caplens)\n",
        "'''\n",
        "def collate_fn_caplen(batch):\n",
        "    imgs = [item[0].unsqueeze(0) for item in batch]\n",
        "    img = torch.cat(imgs, dim=0)\n",
        "    targets = [item[1].unsqueeze(0) for item in batch]\n",
        "    target = torch.cat(targets, dim=0)\n",
        "    caplens = [item[2] for item in batch]\n",
        "    return img, target, torch.LongTensor(caplens)\n",
        "\n",
        "\n",
        "def getLoader(root_dir=\"./data/flickr8k/train/Images\", caption_path=\"./data/flickr8k/train/captions.txt\", word_frequency=5, max_cap_len=50, transform=None, batch_size=32, num_workers=0, shuffle=True, pin_memory=True):\n",
        "    dataset = FlickrDataset(root_dir=root_dir,caption_path=caption_path, freq_threshold=word_frequency, max_cap_len=max_cap_len, transform=transform)\n",
        "    pad_value = dataset.vocab.stoi[\"<PAD>\"]\n",
        "    \n",
        "    #loader = DataLoader(dataset=dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=True, collate_fn=MyCollate(pad_value))\n",
        "    loader = DataLoader(dataset=dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=True, collate_fn=collate_fn_caplen)\n",
        "    return loader, dataset     "
      ],
      "metadata": {
        "id": "c3WvwUnmjD3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean, std = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n",
        "img_size = 256\n",
        "\n",
        "train_transform = transforms.Compose([ \n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.RandomHorizontalFlip(), \n",
        "    transforms.ToTensor(), \n",
        "    transforms.Normalize(mean, std)])\n",
        "\n",
        "train_loader, train_dataset = getLoader(root_dir=train_image_dir, \n",
        "                                         caption_path=train_caption_path, \n",
        "                                         word_frequency=5,\n",
        "                                         transform=train_transform, \n",
        "                                         batch_size=10, \n",
        "                                         num_workers=0, \n",
        "                                         shuffle=True, \n",
        "                                         pin_memory=True)\n"
      ],
      "metadata": {
        "id": "jBGXfs_-UThu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1443a8f4-76d1-4d88-a605-d7398d95bbe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imgs, captions, caplens = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "KxTYiMiKafjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(imgs.shape)\n",
        "print(captions.shape)\n",
        "print(caplens.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNZ-s117cI2d",
        "outputId": "a6fb2086-67b7-4759-acc0-3ac9b0b5de75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 3, 256, 256])\n",
            "torch.Size([10, 52])\n",
            "torch.Size([10, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "caplens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YowMAsNclzXU",
        "outputId": "7090f644-d834-433d-e8a9-17e6293f7c18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([11, 13, 21, 10, 17, 12, 11,  8, 11,  9])]"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 3\n",
        "x = imgs[idx]\n",
        "y = captions[idx]\n",
        "print(caplens[idx])\n",
        "plt.imshow(x.permute(1,2,0))\n",
        "print(y)\n",
        "for i in y:\n",
        "    print(train_dataset.vocab.itos[int(i)],end=\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "A0NdD6wiLI3Z",
        "outputId": "b0b9b65c-809d-41cd-82d7-1178b4c19a7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-6e929bb0c3e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaplens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model"
      ],
      "metadata": {
        "id": "pqSLpR5MW_fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.stoi[\"<PAD>\"])\n",
        "criterion1 = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.stoi[\"<PAD>\"])\n",
        "\n",
        "vocab_size=len(train_dataset.vocab)\n",
        "\n",
        "model = Custom_Transformer(img_size = config.img_size,\n",
        "                            patch_size = config.patch_size,\n",
        "                            in_chans = config.in_chans,\n",
        "                            embed_dim = config.embed_dim,\n",
        "                            vocab_size = vocab_size,\n",
        "                            encoder_depth = config.encoder_depth,\n",
        "                            decoder_depth = config.decoder_depth,\n",
        "                            encoder_heads = config.encoder_heads,\n",
        "                            decoder_heads = config.decoder_heads,\n",
        "                            mlp_ratio = config.mlp_ratio,\n",
        "                            pos_p = config.pos_p,\n",
        "                            attn_p = config.attn_p,\n",
        "                            ffn_p = config.ffn_p)\n",
        "    \n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = config.learning_rate)\n",
        "\n",
        "total_params = sum(\n",
        "    param.numel() for param in model.parameters()\n",
        ")\n",
        "print(f\"Current Model has {total_params} parameters\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "print(f\"Running Model on {device} device\")\n",
        "\n",
        "epoch_train_loss = 0\n",
        "optimizer.zero_grad()\n",
        "imgs = imgs.to(device)\n",
        "#captions = captions\n",
        "\n",
        "# Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
        "y_input = captions[:,:-1]\n",
        "y_expected = captions[:,1:]\n",
        "tgt_mask = model.create_tgt_mask(y_input).to(device)\n",
        "y_input = y_input.to(device)\n",
        "y_expected = y_expected.to(device)\n",
        "print(f\"y_expected shape: {y_expected.shape}\")\n",
        "\n",
        "# Forward\n",
        "output = model(imgs, y_input, trg_mask=tgt_mask)\n",
        "print(f\"output shape: {output.shape}\")\n",
        "output_flat = output.view(-1, vocab_size)\n",
        "print(f\"output_flat shape: {output_flat.shape}\")\n",
        "            \n",
        "train_loss = criterion(output.reshape(-1, output.shape[2]), y_expected.reshape(-1))\n",
        "train_loss_1 = criterion1(output_flat, y_expected)\n",
        "\n",
        "# Backward\n",
        "train_loss.backward()\n",
        "optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "gqPQkFVEFvrA",
        "outputId": "bf5cb079-1619-407c-cf45-64a09a0d358f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Model has 32441328 parameters\n",
            "Running Model on cuda device\n",
            "y_expected shape: torch.Size([10, 14])\n",
            "output shape: torch.Size([10, 14, 2544])\n",
            "output_flat shape: torch.Size([140, 2544])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-5f1049f6c1cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_expected\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mtrain_loss_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_expected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# Backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m   1165\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3013\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3014\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (140) to match target batch_size (10)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Fpe-3AVKOXas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Train the model"
      ],
      "metadata": {
        "id": "O0r-X1HTrzea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(state, filename = \"my_checkpoint.pth.tar\"):\n",
        "    print(\"saving checkpoint!\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def load_checkpoint(checkpoint, model, optimizer):\n",
        "    print(\"loading checkpoint!\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "    step = checkpoint[\"step\"]\n",
        "    return model, optimizer, step\n"
      ],
      "metadata": {
        "id": "hRIhQoq9rnlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "def train(config):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    \n",
        "    # Make a directory that a learned model will be saved\n",
        "    if not os.path.exists(config.models_path):\n",
        "        os.makedirs(config.models_path)\n",
        "\n",
        "    # Make transforms for training, validating, and testing the model\n",
        "    mean, std = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n",
        "    train_transform = transforms.Compose([ \n",
        "        transforms.Resize((config.img_size, config.img_size)),\n",
        "        transforms.RandomHorizontalFlip(), \n",
        "        transforms.ToTensor(), \n",
        "        transforms.Normalize(mean, std)])\n",
        "\n",
        "    val_transform = transforms.Compose([ \n",
        "        transforms.Resize((config.img_size, config.img_size)),\n",
        "        transforms.ToTensor(), \n",
        "        transforms.Normalize(mean, std)])\n",
        "\n",
        "    test_transform = transforms.Compose([ \n",
        "        transforms.Resize((config.img_size, config.img_size)),\n",
        "        transforms.ToTensor(), \n",
        "        transforms.Normalize(mean, std)])\n",
        "    \n",
        "    num_workers = 4 if torch.cuda.is_available() else 0 #colab gpu allows up to 4 workers. laptop cpu only allows 0 worker\n",
        "    word_frequency = 5\n",
        "\n",
        "    # Get the dataloaders\n",
        "    train_image_dir = './data/flickr8k/train/Images' # train images folder\n",
        "    val_image_dir = './data/flickr8k/val/Images' # validation images folder\n",
        "    test_image_dir = './data/flickr8k/test/Images' # test images folder\n",
        "    train_caption_path = \"./data/flickr8k/train/captions.txt\" # train images captions\n",
        "    val_caption_path = \"./data/flickr8k/val/captions.txt\" # validation images captions\n",
        "    test_caption_path = \"./data/flickr8k/test/captions.txt\" # test images captions\n",
        "\n",
        "    train_loader, train_dataset = getLoader(root_dir=train_image_dir, \n",
        "                                            caption_path=train_caption_path, \n",
        "                                            word_frequency=word_frequency,\n",
        "                                            transform=train_transform, \n",
        "                                            batch_size=config.batch_size, \n",
        "                                            num_workers=num_workers, \n",
        "                                            shuffle=True, \n",
        "                                            pin_memory=True)\n",
        "\n",
        "    val_loader, val_dataset = getLoader(root_dir=val_image_dir, \n",
        "                                            caption_path=val_caption_path, \n",
        "                                            word_frequency=word_frequency,                                    \n",
        "                                            transform=val_transform, \n",
        "                                            batch_size=config.batch_size, \n",
        "                                            num_workers=num_workers, \n",
        "                                            shuffle=True, \n",
        "                                            pin_memory=True)\n",
        "\n",
        "    test_loader, test_dataset = getLoader(root_dir=test_image_dir, \n",
        "                                            caption_path=test_caption_path, \n",
        "                                            word_frequency=word_frequency,                                      \n",
        "                                            transform=test_transform, \n",
        "                                            batch_size=config.batch_size, \n",
        "                                            num_workers=num_workers, \n",
        "                                            shuffle=True, \n",
        "                                            pin_memory=True)\n",
        "\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.stoi[\"<PAD>\"])\n",
        "\n",
        "    vocab_size=len(train_dataset.vocab)\n",
        "\n",
        "    model = Custom_Transformer(img_size = config.img_size,\n",
        "                            patch_size = config.patch_size,\n",
        "                            in_chans = config.in_chans,\n",
        "                            embed_dim = config.embed_dim,\n",
        "                            vocab_size = vocab_size,\n",
        "                            encoder_depth = config.encoder_depth,\n",
        "                            decoder_depth = config.decoder_depth,\n",
        "                            encoder_heads = config.encoder_heads,\n",
        "                            decoder_heads = config.decoder_heads,\n",
        "                            mlp_ratio = config.mlp_ratio,\n",
        "                            pos_p = config.pos_p,\n",
        "                            attn_p = config.attn_p,\n",
        "                            ffn_p = config.ffn_p)\n",
        "    \n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "    # this code is very important! It initialises the parameters with a\n",
        "    # range of values that stops the signal fading or getting too big.\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr = config.learning_rate)\n",
        "\n",
        "    total_params = sum(\n",
        "        param.numel() for param in model.parameters()\n",
        "    )\n",
        "    print(f\"Current Model has {total_params} parameters\")\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    print(f\"Running Model on {device} device\")\n",
        "    \n",
        "    model_save_path = os.path.join(config.models_path, config.saved_model_filename)\n",
        "    loss_save_path = os.path.join(config.models_path, config.saved_loss_filename)\n",
        "    step = 0\n",
        "    loss_dict = dict()\n",
        "    loss_dict['train'] = list()\n",
        "    loss_dict['val'] = list()\n",
        "    \n",
        "\n",
        "    if config.load_model:\n",
        "        model, optimizer, step = load_checkpoint(torch.load(model_save_path), model, optimizer)\n",
        "\n",
        "    for epoch in range(config.EPOCHS):\n",
        "        model.train()\n",
        "        if config.save_model:\n",
        "            checkpoint = {\n",
        "                \"state_dict\": model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "                \"step\": step,\n",
        "            }\n",
        "            save_checkpoint(checkpoint, filename = model_save_path)\n",
        "\n",
        "        # Train\n",
        "        epoch_train_loss = 0\n",
        "        for idx, (imgs, captions) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            imgs = imgs.to(device)\n",
        "            #captions = captions\n",
        "\n",
        "            # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
        "            y_input = captions[:,:-1]\n",
        "            y_expected = captions[:,1:]\n",
        "            tgt_mask = model.create_tgt_mask(y_input).to(device)\n",
        "            y_input = y_input.to(device)\n",
        "            y_expected = y_expected.to(device)\n",
        "\n",
        "            # Forward\n",
        "            output = model(imgs, y_input, trg_mask=tgt_mask)\n",
        "            \n",
        "            train_loss = criterion(output.reshape(-1, output.shape[2]), y_expected.reshape(-1))\n",
        "            epoch_train_loss += train_loss.item()\n",
        "\n",
        "            # Backward\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validate\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            epoch_val_loss = 0\n",
        "            for idx, (imgs, captions) in enumerate(val_loader):\n",
        "                imgs = imgs.to(device)\n",
        "                #captions = captions.to(device)\n",
        "\n",
        "                # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
        "                y_input = captions[:,:-1]\n",
        "                y_expected = captions[:,1:]\n",
        "                tgt_mask = model.create_tgt_mask(y_input).to(device)\n",
        "                y_input = y_input.to(device)\n",
        "                y_expected = y_expected.to(device)             \n",
        "\n",
        "                # Forward\n",
        "                output = model(imgs, y_input, trg_mask=tgt_mask)\n",
        "                \n",
        "                val_loss = criterion(output.reshape(-1, output.shape[2]), y_expected.reshape(-1))\n",
        "                epoch_val_loss += val_loss.item()\n",
        "\n",
        "        step+=1\n",
        "        loss_dict['train'].append(epoch_train_loss)\n",
        "        loss_dict['val'].append(epoch_val_loss)\n",
        "        print(f\"Epoch: {epoch} / Train Loss: {epoch_train_loss} / Validation Loss: {epoch_val_loss}\")\n",
        "\n",
        "    # Save loss history \n",
        "    if config.save_result:\n",
        "        with open(config.saved_loss_filename, 'wb') as f:\n",
        "            pickle.dump(loss_dict, f)\n",
        "\n",
        "    plt.plot(list(range(1, config.EPOCHS+1)), loss_dict['train'], label='Train Loss')\n",
        "    plt.plot(list(range(1, config.EPOCHS+1)), loss_dict['val'], label='Validation Loss')\n",
        "    plt.title('Image Captioning Loss curve')\n",
        "    plt.legend(loc='best')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('loss')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "9jD1OvhtnZMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exp 1"
      ],
      "metadata": {
        "id": "VXA9naVy3GXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config(object):\n",
        "    def __init__(self):\n",
        "        # Load Data\n",
        "        self.img_size = 256\n",
        "        self.batch_size = 32\n",
        "\n",
        "        # Transformer parameters\n",
        "        self.patch_size = 16\n",
        "        self.in_chans = 3\n",
        "        self.mlp_ratio = 2.\n",
        "        self.embed_dim = 256\n",
        "        #self.embed_dim = (self.patch_size**2) * self.in_chans\n",
        "        self.encoder_depth=8\n",
        "        self.encoder_heads=8\n",
        "        self.decoder_depth=4\n",
        "        self.decoder_heads=8\n",
        "        self.pos_p=0.0\n",
        "        self.attn_p=0.2\n",
        "        self.ffn_p=0.1\n",
        "\n",
        "        # For Training and Eval\n",
        "        self.EPOCHS = 20\n",
        "        self.learning_rate = 0.001\n",
        "        assert self.embed_dim % self.encoder_heads == 0, \"embed_dim must be a multiple of encoder_heads\"\n",
        "        assert self.embed_dim % self.decoder_heads == 0, \"embed_dim must be a multiple of decoder_heads\"\n",
        "        \n",
        "        # Save the model and the result\n",
        "        self.load_model = False\n",
        "        self.save_model= True\n",
        "        self.save_result = True\n",
        "        self.models_path = \"./learned_models/\" # model path that learned models will be saved\n",
        "        self.saved_model_filename = \"image_captioning_model_200731.pth.tar\"\n",
        "        self.saved_loss_filename = \"image_captioning_loss_200731.pkl\"\n",
        "\n"
      ],
      "metadata": {
        "id": "4MDV-0H42ty8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config()\n",
        "train(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "q4nOAnRU2683",
        "outputId": "0ee3904c-063b-49dd-8924-a6f93987ef56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30000\n",
            "5000\n",
            "5455\n",
            "Current Model has 8881904 parameters\n",
            "Running Model on cuda device\n",
            "saving checkpoint!\n",
            "Epoch: 0 / Train Loss: 4881.377447128296 / Validation Loss: 972.0333361625671\n",
            "saving checkpoint!\n",
            "Epoch: 1 / Train Loss: 4686.939743041992 / Validation Loss: 1313.405306339264\n",
            "saving checkpoint!\n",
            "Epoch: 2 / Train Loss: 4601.913410186768 / Validation Loss: 1258.5481181144714\n",
            "saving checkpoint!\n",
            "Epoch: 3 / Train Loss: 4581.517372131348 / Validation Loss: 1387.5554113388062\n",
            "saving checkpoint!\n",
            "Epoch: 4 / Train Loss: 4568.786396026611 / Validation Loss: 1306.6097331047058\n",
            "saving checkpoint!\n",
            "Epoch: 5 / Train Loss: 4561.614007472992 / Validation Loss: 1344.1270751953125\n",
            "saving checkpoint!\n",
            "Epoch: 6 / Train Loss: 4552.894637584686 / Validation Loss: 1376.46653175354\n",
            "saving checkpoint!\n",
            "Epoch: 7 / Train Loss: 4548.108090877533 / Validation Loss: 1260.280589580536\n",
            "saving checkpoint!\n",
            "Epoch: 8 / Train Loss: 4542.685781002045 / Validation Loss: 1268.4758071899414\n",
            "saving checkpoint!\n",
            "Epoch: 9 / Train Loss: 4541.55259180069 / Validation Loss: 1344.029764175415\n",
            "saving checkpoint!\n",
            "Epoch: 10 / Train Loss: 4535.18289232254 / Validation Loss: 1277.8574151992798\n",
            "saving checkpoint!\n",
            "Epoch: 11 / Train Loss: 4529.628970146179 / Validation Loss: 1268.4695777893066\n",
            "saving checkpoint!\n",
            "Epoch: 12 / Train Loss: 4525.294148445129 / Validation Loss: 1283.6968169212341\n",
            "saving checkpoint!\n",
            "Epoch: 13 / Train Loss: 4519.51854801178 / Validation Loss: 1246.2140951156616\n",
            "saving checkpoint!\n",
            "Epoch: 14 / Train Loss: 4514.089930534363 / Validation Loss: 1299.0700731277466\n",
            "saving checkpoint!\n",
            "Epoch: 15 / Train Loss: 4509.127201080322 / Validation Loss: 1246.9178795814514\n",
            "saving checkpoint!\n",
            "Epoch: 16 / Train Loss: 4505.976990699768 / Validation Loss: 1293.306561946869\n",
            "saving checkpoint!\n",
            "Epoch: 17 / Train Loss: 4504.983350276947 / Validation Loss: 1302.5367360115051\n",
            "saving checkpoint!\n",
            "Epoch: 18 / Train Loss: 4500.650772571564 / Validation Loss: 1205.823830127716\n",
            "saving checkpoint!\n",
            "Epoch: 19 / Train Loss: 4499.523142337799 / Validation Loss: 1264.2306699752808\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exp 2"
      ],
      "metadata": {
        "id": "ZLX7JW2G3KNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config(object):\n",
        "    def __init__(self):\n",
        "        # Load Data\n",
        "        self.img_size = 256\n",
        "        self.batch_size = 32\n",
        "\n",
        "        # Transformer parameters\n",
        "        self.patch_size = 16\n",
        "        self.in_chans = 3\n",
        "        self.mlp_ratio = 2.\n",
        "        self.embed_dim = 512\n",
        "        #self.embed_dim = (self.patch_size**2) * self.in_chans\n",
        "        self.encoder_depth=8\n",
        "        self.encoder_heads=8\n",
        "        self.decoder_depth=4\n",
        "        self.decoder_heads=8\n",
        "        self.pos_p=0.0\n",
        "        self.attn_p=0.2\n",
        "        self.ffn_p=0.2\n",
        "\n",
        "        # For Training and Eval\n",
        "        self.EPOCHS = 20\n",
        "        self.learning_rate = 0.0001\n",
        "        assert self.embed_dim % self.encoder_heads == 0, \"embed_dim must be a multiple of encoder_heads\"\n",
        "        assert self.embed_dim % self.decoder_heads == 0, \"embed_dim must be a multiple of decoder_heads\"\n",
        "        \n",
        "        # Save the model and the result\n",
        "        self.load_model = False\n",
        "        self.save_model= True\n",
        "        self.save_result = True\n",
        "        self.models_path = \"./learned_models/\" # model path that learned models will be saved\n",
        "        self.saved_model_filename = \"image_captioning_model_200731_2.pth.tar\"\n",
        "        self.saved_loss_filename = \"image_captioning_loss_200731_2.pkl\"\n"
      ],
      "metadata": {
        "id": "9DaNbTQzD-R6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = Config()\n",
        "train(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dGTGJiPzD-Us",
        "outputId": "b2482d59-2e23-47d3-d11d-30418d0fb49b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30000\n",
            "5000\n",
            "5455\n",
            "Current Model has 32441328 parameters\n",
            "Running Model on cuda device\n",
            "saving checkpoint!\n",
            "Epoch: 0 / Train Loss: 4757.369761943817 / Validation Loss: 976.2076320648193\n",
            "saving checkpoint!\n",
            "Epoch: 1 / Train Loss: 4216.286834716797 / Validation Loss: 1048.0867638587952\n",
            "saving checkpoint!\n",
            "Epoch: 2 / Train Loss: 3727.9979457855225 / Validation Loss: 1108.2648072242737\n",
            "saving checkpoint!\n",
            "Epoch: 3 / Train Loss: 3500.515816926956 / Validation Loss: 1128.541271686554\n",
            "saving checkpoint!\n",
            "Epoch: 4 / Train Loss: 3358.3692150115967 / Validation Loss: 1149.5333189964294\n",
            "saving checkpoint!\n",
            "Epoch: 5 / Train Loss: 3235.92457985878 / Validation Loss: 1164.117516040802\n",
            "saving checkpoint!\n",
            "Epoch: 6 / Train Loss: 3140.1394896507263 / Validation Loss: 1190.2466039657593\n",
            "saving checkpoint!\n",
            "Epoch: 7 / Train Loss: 3052.7343957424164 / Validation Loss: 1205.2176432609558\n",
            "saving checkpoint!\n",
            "Epoch: 8 / Train Loss: 2972.8715918064117 / Validation Loss: 1197.5930891036987\n",
            "saving checkpoint!\n",
            "Epoch: 9 / Train Loss: 2911.9930713176727 / Validation Loss: 1243.3191118240356\n",
            "saving checkpoint!\n",
            "Epoch: 10 / Train Loss: 2854.0168628692627 / Validation Loss: 1244.9452347755432\n",
            "saving checkpoint!\n",
            "Epoch: 11 / Train Loss: 2803.1942903995514 / Validation Loss: 1257.374836921692\n",
            "saving checkpoint!\n",
            "Epoch: 12 / Train Loss: 2749.149235725403 / Validation Loss: 1265.0961289405823\n",
            "saving checkpoint!\n",
            "Epoch: 13 / Train Loss: 2714.637940645218 / Validation Loss: 1272.0702729225159\n",
            "saving checkpoint!\n",
            "Epoch: 14 / Train Loss: 2671.2831177711487 / Validation Loss: 1262.23827791214\n",
            "saving checkpoint!\n",
            "Epoch: 15 / Train Loss: 2641.02210354805 / Validation Loss: 1280.929615020752\n",
            "saving checkpoint!\n",
            "Epoch: 16 / Train Loss: 2606.8670794963837 / Validation Loss: 1287.274333000183\n",
            "saving checkpoint!\n",
            "Epoch: 17 / Train Loss: 2567.8866686820984 / Validation Loss: 1292.9889912605286\n",
            "saving checkpoint!\n",
            "Epoch: 18 / Train Loss: 2549.832040786743 / Validation Loss: 1292.3501553535461\n",
            "saving checkpoint!\n",
            "Epoch: 19 / Train Loss: 2509.547883272171 / Validation Loss: 1297.7779688835144\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exp 3"
      ],
      "metadata": {
        "id": "-QHyk992Gw2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean, std = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n",
        "img_size = 384\n",
        "\n",
        "# Make transforms for training, validating, and testing the model\n",
        "train_transform = transforms.Compose([ \n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.RandomHorizontalFlip(), \n",
        "    transforms.ToTensor(), \n",
        "    transforms.Normalize(mean, std)])\n",
        "\n",
        "val_transform = transforms.Compose([ \n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.ToTensor(), \n",
        "    transforms.Normalize(mean, std)])\n",
        "\n",
        "test_transform = transforms.Compose([ \n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.ToTensor(), \n",
        "    transforms.Normalize(mean, std)])\n",
        "\n",
        "batch_size = 32\n",
        "num_workers = 4\n",
        "word_frequency = 5\n",
        "\n",
        "# Get the dataloaders\n",
        "train_loader, train_dataset = getLoader(root_dir=train_image_dir, \n",
        "                                        caption_path=train_caption_path, \n",
        "                                        word_frequency=word_frequency,\n",
        "                                        transform=train_transform, \n",
        "                                        batch_size=batch_size, \n",
        "                                        num_workers=num_workers, \n",
        "                                        shuffle=True, \n",
        "                                        pin_memory=True)\n",
        "\n",
        "val_loader, val_dataset = getLoader(root_dir=val_image_dir, \n",
        "                                        caption_path=val_caption_path, \n",
        "                                        word_frequency=word_frequency,                                    \n",
        "                                        transform=val_transform, \n",
        "                                        batch_size=batch_size, \n",
        "                                        num_workers=num_workers, \n",
        "                                        shuffle=True, \n",
        "                                        pin_memory=True)\n",
        "\n",
        "test_loader, test_dataset = getLoader(root_dir=test_image_dir, \n",
        "                                        caption_path=test_caption_path, \n",
        "                                        word_frequency=word_frequency,                                      \n",
        "                                        transform=test_transform, \n",
        "                                        batch_size=batch_size, \n",
        "                                        num_workers=num_workers, \n",
        "                                        shuffle=True, \n",
        "                                        pin_memory=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kRGpdSdGzxa",
        "outputId": "a7606ac1-a9e5-4b29-d637-1b564bcbce45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30000\n",
            "5000\n",
            "5455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "learning_rate = 0.001\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.vocab.stoi[\"<PAD>\"])\n",
        "\n",
        "# Example of Transformer\n",
        "patch_size = 16\n",
        "in_chans = 3\n",
        "mlp_ratio = 2.\n",
        "embed_dim = 256\n",
        "#embed_dim = (patch_size**2) * in_chans\n",
        "hidden_features = int(embed_dim * mlp_ratio)\n",
        "encoder_depth=12\n",
        "encoder_heads=8\n",
        "decoder_depth=4\n",
        "decoder_heads=8\n",
        "pos_p=0.0\n",
        "attn_p=0.1\n",
        "ffn_p=0.1\n",
        "vocab_size=len(train_dataset.vocab)\n",
        "\n",
        "model = Custom_Transformer(img_size = img_size,\n",
        "                           patch_size = patch_size,\n",
        "                           in_chans = in_chans,\n",
        "                           embed_dim = embed_dim,\n",
        "                           vocab_size = vocab_size,\n",
        "                           encoder_depth = encoder_depth,\n",
        "                           decoder_depth = decoder_depth,\n",
        "                           encoder_heads = encoder_heads,\n",
        "                           decoder_heads = decoder_heads,\n",
        "                           mlp_ratio = mlp_ratio,\n",
        "                           pos_p = pos_p,\n",
        "                           attn_p = attn_p,\n",
        "                           ffn_p = ffn_p)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "total_params = sum(\n",
        "\tparam.numel() for param in model.parameters()\n",
        ")\n",
        "print(f\"Current Model has {total_params} parameters\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "print(f\"Running Model on {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjO2E0YsG9Mi",
        "outputId": "bf1e2ddf-567b-436d-cb64-5e9428b50c2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Model has 13647660 parameters\n",
            "Running Model on cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_model = False\n",
        "save_model= True\n",
        "save_result = True\n",
        "saved_model_path = os.path.join(models_path, \"image_captioning_model_200731.pth.tar\")\n",
        "step = 0\n",
        "loss_dict = dict()\n",
        "loss_dict['train'] = list()\n",
        "loss_dict['val'] = list()\n",
        "loss_path = os.path.join(models_path, \"image_captioning_loss_200731.pkl\")\n",
        "\n",
        "if load_model:\n",
        "    model, optimizer, step = load_checkpoint(torch.load(saved_model_path), model, optimizer)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    if save_model:\n",
        "        checkpoint = {\n",
        "            \"state_dict\": model.state_dict(),\n",
        "            \"optimizer\": optimizer.state_dict(),\n",
        "            \"step\": step,\n",
        "        }\n",
        "        save_checkpoint(checkpoint, filename = saved_model_path)\n",
        "\n",
        "    # Train\n",
        "    epoch_train_loss = 0\n",
        "    for idx, (imgs, captions) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        imgs = imgs.to(device)\n",
        "        captions = captions.to(device)\n",
        "\n",
        "        # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
        "        y_input = captions[:,:-1]\n",
        "        y_expected = captions[:,1:]\n",
        "\n",
        "        # Forward\n",
        "        output = model(imgs, y_input)\n",
        "        \n",
        "        train_loss = criterion(output.reshape(-1, output.shape[2]), y_expected.reshape(-1))\n",
        "        epoch_train_loss += train_loss.item()\n",
        "\n",
        "        # Backward\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validate\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        epoch_val_loss = 0\n",
        "        for idx, (imgs, captions) in enumerate(val_loader):\n",
        "            imgs = imgs.to(device)\n",
        "            captions = captions.to(device)\n",
        "\n",
        "            # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
        "            y_input = captions[:,:-1]\n",
        "            y_expected = captions[:,1:]\n",
        "\n",
        "            # Forward\n",
        "            output = model(imgs, y_input)\n",
        "            \n",
        "            val_loss = criterion(output.reshape(-1, output.shape[2]), y_expected.reshape(-1))\n",
        "            epoch_val_loss += val_loss.item()\n",
        "\n",
        "    step+=1\n",
        "    loss_dict['train'].append(epoch_train_loss)\n",
        "    loss_dict['val'].append(epoch_val_loss)\n",
        "    print(f\"Epoch: {epoch} / Train Loss: {epoch_train_loss} / Validation Loss: {epoch_val_loss}\")\n",
        "\n",
        "# Save loss history \n",
        "if save_result:\n",
        "    with open(loss_path, 'wb') as f:\n",
        "        pickle.dump(loss_dict, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzYjJFlEHMBS",
        "outputId": "173046d0-47be-414f-f765-b3ec2479ce75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving checkpoint!\n",
            "Epoch: 0 / Train Loss: 2550.8919488191605 / Validation Loss: 1009.6061401367188\n",
            "saving checkpoint!\n",
            "Epoch: 1 / Train Loss: 1467.3484588861465 / Validation Loss: 992.7778663635254\n",
            "saving checkpoint!\n",
            "Epoch: 2 / Train Loss: 1126.6613512039185 / Validation Loss: 1017.9664421081543\n",
            "saving checkpoint!\n",
            "Epoch: 3 / Train Loss: 943.8435942530632 / Validation Loss: 1019.9210410118103\n",
            "saving checkpoint!\n",
            "Epoch: 4 / Train Loss: 840.3898208141327 / Validation Loss: 1033.7683420181274\n",
            "saving checkpoint!\n",
            "Epoch: 5 / Train Loss: 778.9295016527176 / Validation Loss: 1087.3031611442566\n",
            "saving checkpoint!\n",
            "Epoch: 6 / Train Loss: 729.5199763178825 / Validation Loss: 1079.2211027145386\n",
            "saving checkpoint!\n",
            "Epoch: 7 / Train Loss: 692.3223264217377 / Validation Loss: 1100.2316823005676\n",
            "saving checkpoint!\n",
            "Epoch: 8 / Train Loss: 664.6382794082165 / Validation Loss: 1127.5831723213196\n",
            "saving checkpoint!\n",
            "Epoch: 9 / Train Loss: 639.8651024103165 / Validation Loss: 1133.2075867652893\n",
            "saving checkpoint!\n",
            "Epoch: 10 / Train Loss: 616.8589035272598 / Validation Loss: 1182.104506969452\n",
            "saving checkpoint!\n",
            "Epoch: 11 / Train Loss: 598.8659817278385 / Validation Loss: 1174.2352356910706\n",
            "saving checkpoint!\n",
            "Epoch: 12 / Train Loss: 582.8476184904575 / Validation Loss: 1175.839243888855\n",
            "saving checkpoint!\n",
            "Epoch: 13 / Train Loss: 571.4256086945534 / Validation Loss: 1237.9920353889465\n",
            "saving checkpoint!\n",
            "Epoch: 14 / Train Loss: 554.066479742527 / Validation Loss: 1220.712209224701\n",
            "saving checkpoint!\n",
            "Epoch: 15 / Train Loss: 544.9851243197918 / Validation Loss: 1241.3303923606873\n",
            "saving checkpoint!\n",
            "Epoch: 16 / Train Loss: 531.7657918930054 / Validation Loss: 1251.6322512626648\n",
            "saving checkpoint!\n",
            "Epoch: 17 / Train Loss: 521.3541854023933 / Validation Loss: 1246.7423424720764\n",
            "saving checkpoint!\n",
            "Epoch: 18 / Train Loss: 512.3035690188408 / Validation Loss: 1251.3762683868408\n",
            "saving checkpoint!\n",
            "Epoch: 19 / Train Loss: 502.03285160660744 / Validation Loss: 1285.0822429656982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(list(range(1, EPOCHS+1)), loss_dict['train'], label='Train Loss')\n",
        "plt.plot(list(range(1, EPOCHS+1)), loss_dict['val'], label='Validation Loss')\n",
        "plt.title('Image Captioning Loss curve')\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "4FJnUAHYHMEw",
        "outputId": "91497ee6-7b4e-4f63-b5a5-27af45f2f2c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), os.path.join(models_path, \"my_cptr_200731.ckpt\"))"
      ],
      "metadata": {
        "id": "XKS3o6ZGHZyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exp 4"
      ],
      "metadata": {
        "id": "_RQZKaPXcfPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "channel_number = 512\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, QKVdim):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.QKVdim = QKVdim\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        \"\"\"\n",
        "        :param Q: [batch_size, n_heads, -1(len_q), QKVdim]\n",
        "        :param K, V: [batch_size, n_heads, -1(len_k=len_v), QKVdim]\n",
        "        :param attn_mask: [batch_size, n_heads, len_q, len_k]\n",
        "        \"\"\"\n",
        "        # scores: [batch_size, n_heads, len_q, len_k]\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(self.QKVdim)\n",
        "        # Fills elements of self tensor with value where mask is True.\n",
        "        scores.to(device).masked_fill_(attn_mask, -1e9)\n",
        "        attn = nn.Softmax(dim=-1)(scores)  # [batch_size, n_heads, len_q, len_k]\n",
        "        context = torch.matmul(attn, V).to(device)  # [batch_size, n_heads, len_q, QKVdim]\n",
        "        return context, attn\n",
        "\n",
        "\n",
        "class Multi_Head_Attention(nn.Module):\n",
        "    def __init__(self, Q_dim, K_dim, QKVdim, n_heads=8, dropout=0.1):\n",
        "        super(Multi_Head_Attention, self).__init__()\n",
        "        self.W_Q = nn.Linear(Q_dim, QKVdim * n_heads).to(device)\n",
        "        self.W_K = nn.Linear(K_dim, QKVdim * n_heads).to(device)\n",
        "        self.W_V = nn.Linear(K_dim, QKVdim * n_heads).to(device)\n",
        "        self.n_heads = n_heads\n",
        "        self.QKVdim = QKVdim\n",
        "        self.embed_dim = Q_dim\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.W_O = nn.Linear(self.n_heads * self.QKVdim, self.embed_dim).to(device)\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        \"\"\"\n",
        "        In self-encoder attention:\n",
        "                Q = K = V: [batch_size, num_pixels=196, encoder_dim=2048]\n",
        "                attn_mask: [batch_size, len_q=196, len_k=196]\n",
        "        In self-decoder attention:\n",
        "                Q = K = V: [batch_size, max_len=52, embed_dim=512]\n",
        "                attn_mask: [batch_size, len_q=52, len_k=52]\n",
        "        encoder-decoder attention:\n",
        "                Q: [batch_size, 52, 512] from decoder\n",
        "                K, V: [batch_size, 196, 2048] from encoder\n",
        "                attn_mask: [batch_size, len_q=52, len_k=196]\n",
        "        return _, attn: [batch_size, n_heads, len_q, len_k]\n",
        "        \"\"\"\n",
        "        residual, batch_size = Q, Q.size(0)\n",
        "        # q_s: [batch_size, n_heads=8, len_q, QKVdim] k_s/v_s: [batch_size, n_heads=8, len_k, QKVdim]\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.QKVdim).transpose(1, 2)\n",
        "        k_s = self.W_K(K).view(batch_size, -1, self.n_heads, self.QKVdim).transpose(1, 2)\n",
        "        v_s = self.W_V(V).view(batch_size, -1, self.n_heads, self.QKVdim).transpose(1, 2)\n",
        "        print(f\"Q: {Q.shape}, K: {K.shape}, V: {V.shape}\")\n",
        "        print(f\"q_s: {q_s.shape}, k_s: {k_s.shape}, v_s: {v_s.shape}\")\n",
        "        # attn_mask: [batch_size, self.n_heads, len_q, len_k]\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1)\n",
        "        # attn: [batch_size, n_heads, len_q, len_k]\n",
        "        # context: [batch_size, n_heads, len_q, QKVdim]\n",
        "        context, attn = ScaledDotProductAttention(self.QKVdim)(q_s, k_s, v_s, attn_mask)\n",
        "        # context: [batch_size, n_heads, len_q, QKVdim] -> [batch_size, len_q, n_heads * QKVdim]\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.QKVdim).to(device)\n",
        "        # output: [batch_size, len_q, embed_dim]\n",
        "        output = self.W_O(context)\n",
        "        output = self.dropout(output)\n",
        "        print(\"After output + residual in MHA: \", nn.LayerNorm(self.embed_dim).to(device)(output + residual).shape)\n",
        "        return nn.LayerNorm(self.embed_dim).to(device)(output + residual), attn\n",
        "\n",
        "\n",
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self, embed_dim, d_ff, dropout):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        \"\"\"\n",
        "        Two fc layers can also be described by two cnn with kernel_size=1.\n",
        "        \"\"\"\n",
        "        self.conv1 = nn.Conv1d(in_channels=embed_dim, out_channels=d_ff, kernel_size=1).to(device)\n",
        "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=embed_dim, kernel_size=1).to(device)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        encoder: inputs: [batch_size, len_q=196, embed_dim=2048]\n",
        "        decoder: inputs: [batch_size, max_len=52, embed_dim=512]\n",
        "        \"\"\"\n",
        "        residual = inputs\n",
        "        output = nn.ReLU()(self.conv1(inputs.transpose(1, 2)))\n",
        "        output = self.conv2(output).transpose(1, 2)\n",
        "        output = self.dropout(output)\n",
        "        return nn.LayerNorm(self.embed_dim).to(device)(output + residual)\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, dropout, attention_method, n_heads):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.dec_self_attn = Multi_Head_Attention(Q_dim=embed_dim, K_dim=embed_dim, QKVdim=64, n_heads=n_heads, dropout=dropout)\n",
        "        if attention_method == \"ByPixel\":\n",
        "            self.dec_enc_attn = Multi_Head_Attention(Q_dim=embed_dim, K_dim=2048, QKVdim=64, n_heads=n_heads, dropout=dropout)\n",
        "            self.pos_ffn = PoswiseFeedForwardNet(embed_dim=embed_dim, d_ff=2048, dropout=dropout)\n",
        "        elif attention_method == \"ByChannel\":\n",
        "            self.dec_enc_attn = Multi_Head_Attention(Q_dim=embed_dim, K_dim=196, QKVdim=64, n_heads=n_heads, dropout=dropout)\n",
        "            self.pos_ffn = PoswiseFeedForwardNet(embed_dim=embed_dim, d_ff=2048, dropout=dropout)  # need to change\n",
        "\n",
        "    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n",
        "        \"\"\"\n",
        "        :param dec_inputs: [batch_size, max_len=52, embed_dim=512]\n",
        "        :param enc_outputs: [batch_size, num_pixels=196, 2048]\n",
        "        :param dec_self_attn_mask: [batch_size, 52, 52]\n",
        "        :param dec_enc_attn_mask: [batch_size, 52, 196]\n",
        "        \"\"\"\n",
        "        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)\n",
        "        print(\"Decoder: Output of Self Attention: \", dec_outputs.shape)\n",
        "        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n",
        "        print(\"Decoder: Output of Encoder-Decoder Attention: \", dec_outputs.shape)\n",
        "        dec_outputs = self.pos_ffn(dec_outputs)\n",
        "        print(\"Decoder: Output of FFN: \", dec_outputs.shape)\n",
        "        return dec_outputs, dec_self_attn, dec_enc_attn\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, n_layers, vocab_size, embed_dim, dropout, attention_method, n_heads):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tgt_emb = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.pos_emb = nn.Embedding.from_pretrained(self.get_position_embedding_table(embed_dim), freeze=True)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(embed_dim, dropout, attention_method, n_heads) for _ in range(n_layers)])\n",
        "        self.projection = nn.Linear(embed_dim, vocab_size, bias=False).to(device)\n",
        "        self.attention_method = attention_method\n",
        "\n",
        "    def get_position_embedding_table(self, embed_dim):\n",
        "        def cal_angle(position, hid_idx):\n",
        "            return position / np.power(10000, 2 * (hid_idx // 2) / embed_dim)\n",
        "        def get_posi_angle_vec(position):\n",
        "            return [cal_angle(position, hid_idx) for hid_idx in range(embed_dim)]\n",
        "\n",
        "        embedding_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(52)])\n",
        "        embedding_table[:, 0::2] = np.sin(embedding_table[:, 0::2])  # dim 2i\n",
        "        embedding_table[:, 1::2] = np.cos(embedding_table[:, 1::2])  # dim 2i+1\n",
        "        return torch.FloatTensor(embedding_table).to(device)\n",
        "\n",
        "    def get_attn_pad_mask(self, seq_q, seq_k):\n",
        "        batch_size, len_q = seq_q.size()\n",
        "        batch_size, len_k = seq_k.size()\n",
        "        # In wordmap, <pad>:0\n",
        "        # pad_attn_mask: [batch_size, 1, len_k], one is masking\n",
        "        pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)\n",
        "        return pad_attn_mask.expand(batch_size, len_q, len_k)  # [batch_size, len_q, len_k]\n",
        "\n",
        "    def get_attn_subsequent_mask(self, seq):\n",
        "        attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
        "        subsequent_mask = np.triu(np.ones(attn_shape), k=1)\n",
        "        subsequent_mask = torch.from_numpy(subsequent_mask).byte().to(device)\n",
        "        return subsequent_mask\n",
        "\n",
        "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
        "        \"\"\"\n",
        "        :param encoder_out: [batch_size, num_pixels=196, 2048]\n",
        "        :param encoded_captions: [batch_size, 52]\n",
        "        :param caption_lengths: [batch_size, 1]\n",
        "        \"\"\"\n",
        "        batch_size = encoder_out.size(0)\n",
        "        # Sort input data by decreasing lengths.\n",
        "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
        "        encoder_out = encoder_out[sort_ind]\n",
        "        encoded_captions = encoded_captions[sort_ind]\n",
        "        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
        "        # So, decoding lengths are actual lengths - 1\n",
        "        print(\"caption lengths in decoder :\", caption_lengths)\n",
        "        decode_lengths = (caption_lengths - 1).tolist()\n",
        "        print(\"decode  lengths in decoder :\", decode_lengths)\n",
        "\n",
        "        # dec_outputs: [batch_size, max_len=52, embed_dim=512]\n",
        "        # dec_self_attn_pad_mask: [batch_size, len_q=52, len_k=52], 1 if id=0(<pad>)\n",
        "        # dec_self_attn_subsequent_mask: [batch_size, 52, 52], Upper triangle of an array with 1.\n",
        "        # dec_self_attn_mask for self-decoder attention, the position whose val > 0 will be masked.\n",
        "        # dec_enc_attn_mask for encoder-decoder attention.\n",
        "        # e.g. 9488, 23, 53, 74, 0, 0  |  dec_self_attn_mask:\n",
        "        # 0 1 1 1 2 2\n",
        "        # 0 0 1 1 2 2\n",
        "        # 0 0 0 1 2 2\n",
        "        # 0 0 0 0 2 2\n",
        "        # 0 0 0 0 1 2\n",
        "        # 0 0 0 0 1 1\n",
        "        print(\"Output of Target Embedding: \", self.tgt_emb(encoded_captions).shape)\n",
        "        print(\"Output after Positional Encoding: \", self.pos_emb(torch.LongTensor([list(range(52))]*batch_size).to(device)).shape)\n",
        "        dec_outputs = self.tgt_emb(encoded_captions) + self.pos_emb(torch.LongTensor([list(range(52))]*batch_size).to(device))\n",
        "        print(\"Output after Target Embedding and Positional Encoding: \", dec_outputs.shape)\n",
        "        \n",
        "        dec_outputs = self.dropout(dec_outputs)\n",
        "        dec_self_attn_pad_mask = self.get_attn_pad_mask(encoded_captions, encoded_captions)\n",
        "        dec_self_attn_subsequent_mask = self.get_attn_subsequent_mask(encoded_captions)\n",
        "        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequent_mask), 0)\n",
        "        if self.attention_method == \"ByPixel\":\n",
        "            dec_enc_attn_mask = (torch.tensor(np.zeros((batch_size, 52, 196))).to(device) == torch.tensor(np.ones((batch_size, 52, 196))).to(device))\n",
        "        elif self.attention_method == \"ByChannel\":\n",
        "            dec_enc_attn_mask = (torch.tensor(np.zeros((batch_size, 52, channel_number))).to(device) == torch.tensor(np.ones((batch_size, 52, channel_number))).to(device))\n",
        "\n",
        "        dec_self_attns, dec_enc_attns = [], []\n",
        "        for layer in self.layers:\n",
        "            # attn: [batch_size, n_heads, len_q, len_k]\n",
        "            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, encoder_out, dec_self_attn_mask, dec_enc_attn_mask)\n",
        "            dec_self_attns.append(dec_self_attn)\n",
        "            dec_enc_attns.append(dec_enc_attn)\n",
        "        print(\"Output after Decode Layers: \", dec_outputs.shape)\n",
        "        predictions = self.projection(dec_outputs)\n",
        "        print(\"Output after Projection Layers: \", predictions.shape)\n",
        "        return predictions, encoded_captions, decode_lengths, sort_ind, dec_self_attns, dec_enc_attns\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, dropout, attention_method, n_heads):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        \"\"\"\n",
        "        In \"Attention is all you need\" paper, dk = dv = 64, h = 8, N=6\n",
        "        \"\"\"\n",
        "        if attention_method == \"ByPixel\":\n",
        "            self.enc_self_attn = Multi_Head_Attention(Q_dim=2048, K_dim=2048, QKVdim=64, n_heads=n_heads, dropout=dropout)\n",
        "            self.pos_ffn = PoswiseFeedForwardNet(embed_dim=2048, d_ff=4096, dropout=dropout)\n",
        "        elif attention_method == \"ByChannel\":\n",
        "            self.enc_self_attn = Multi_Head_Attention(Q_dim=196, K_dim=196, QKVdim=64, n_heads=n_heads, dropout=dropout)\n",
        "            self.pos_ffn = PoswiseFeedForwardNet(embed_dim=196, d_ff=512, dropout=dropout)\n",
        "\n",
        "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
        "        \"\"\"\n",
        "        :param enc_inputs: [batch_size, num_pixels=196, 2048]\n",
        "        :param enc_outputs: [batch_size, len_q=196, d_model=2048]\n",
        "        :return: attn: [batch_size, n_heads=8, 196, 196]\n",
        "        \"\"\"\n",
        "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)\n",
        "        enc_outputs = self.pos_ffn(enc_outputs)\n",
        "        return enc_outputs, attn\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, n_layers, dropout, attention_method, n_heads):\n",
        "        super(Encoder, self).__init__()\n",
        "        if attention_method == \"ByPixel\":\n",
        "            self.pos_emb = nn.Embedding.from_pretrained(self.get_position_embedding_table(), freeze=True)\n",
        "        # self.dropout = nn.Dropout(p=dropout)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(dropout, attention_method, n_heads) for _ in range(n_layers)])\n",
        "        self.attention_method = attention_method\n",
        "\n",
        "    def get_position_embedding_table(self):\n",
        "        def cal_angle(position, hid_idx):\n",
        "            x = position % 14\n",
        "            y = position // 14\n",
        "            x_enc = x / np.power(10000, hid_idx / 1024)\n",
        "            y_enc = y / np.power(10000, hid_idx / 1024)\n",
        "            return np.sin(x_enc), np.sin(y_enc)\n",
        "        def get_posi_angle_vec(position):\n",
        "            return [cal_angle(position, hid_idx)[0] for hid_idx in range(1024)] + [cal_angle(position, hid_idx)[1] for hid_idx in range(1024)]\n",
        "\n",
        "        embedding_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(196)])\n",
        "        return torch.FloatTensor(embedding_table).to(device)\n",
        "\n",
        "    def forward(self, encoder_out):\n",
        "        \"\"\"\n",
        "        :param encoder_out: [batch_size, num_pixels=196, dmodel=2048]\n",
        "        \"\"\"\n",
        "        batch_size = encoder_out.size(0)\n",
        "        positions = encoder_out.size(1)\n",
        "        if self.attention_method == \"ByPixel\":\n",
        "            encoder_out = encoder_out + self.pos_emb(torch.LongTensor([list(range(positions))]*batch_size).to(device))\n",
        "        # encoder_out = self.dropout(encoder_out)\n",
        "        # enc_self_attn_mask: [batch_size, 196, 196]\n",
        "        enc_self_attn_mask = (torch.tensor(np.zeros((batch_size, positions, positions))).to(device)\n",
        "                              == torch.tensor(np.ones((batch_size, positions, positions))).to(device))\n",
        "        enc_self_attns = []\n",
        "        for layer in self.layers:\n",
        "            encoder_out, enc_self_attn = layer(encoder_out, enc_self_attn_mask)\n",
        "            enc_self_attns.append(enc_self_attn)\n",
        "        \n",
        "        return encoder_out, enc_self_attns\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\"\n",
        "    See paper 5.4: \"Attention Is All You Need\" - https://arxiv.org/abs/1706.03762\n",
        "    \"Apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized.\n",
        "    In addition, apply dropout to the sums of the embeddings and the positional encodings in both the encoder\n",
        "    and decoder stacks.\" (Now, we dont't apply dropout to the encoder embeddings)\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embed_dim, encoder_layers, decoder_layers, dropout=0.1, attention_method=\"ByPixel\", n_heads=8):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = Encoder(encoder_layers, dropout, attention_method, n_heads)\n",
        "        self.decoder = Decoder(decoder_layers, vocab_size, embed_dim, dropout, attention_method, n_heads)\n",
        "        self.embedding = self.decoder.tgt_emb\n",
        "        self.attention_method = attention_method\n",
        "\n",
        "    def load_pretrained_embeddings(self, embeddings):\n",
        "        self.embedding.weight = nn.Parameter(embeddings)\n",
        "\n",
        "    def fine_tune_embeddings(self, fine_tune=True):\n",
        "        for p in self.embedding.parameters():\n",
        "            p.requires_grad = fine_tune\n",
        "\n",
        "    def forward(self, enc_inputs, encoded_captions, caption_lengths):\n",
        "        \"\"\"\n",
        "        preprocess: enc_inputs: [batch_size, 14, 14, 2048]/[batch_size, 196, 2048] -> [batch_size, 196, 2048]\n",
        "        encoded_captions: [batch_size, 52]\n",
        "        caption_lengths: [batch_size, 1], not used\n",
        "        The encoder or decoder is composed of a stack of n_layers=6 identical layers.\n",
        "        One layer in encoder: Multi-head Attention(self-encoder attention) with Norm & Residual\n",
        "                            + Feed Forward with Norm & Residual\n",
        "        One layer in decoder: Masked Multi-head Attention(self-decoder attention) with Norm & Residual\n",
        "                            + Multi-head Attention(encoder-decoder attention) with Norm & Residual\n",
        "                            + Feed Forward with Norm & Residual\n",
        "        \"\"\"\n",
        "        batch_size = enc_inputs.size(0)\n",
        "        encoder_dim = enc_inputs.size(-1)\n",
        "        if self.attention_method == \"ByPixel\":\n",
        "            enc_inputs = enc_inputs.view(batch_size, -1, encoder_dim)\n",
        "        elif self.attention_method == \"ByChannel\":\n",
        "            enc_inputs = enc_inputs.view(batch_size, -1, encoder_dim).permute(0, 2, 1)  # (batch_size, 2048, 196)\n",
        "\n",
        "        print(\"** Running Transformer Encoder\")\n",
        "        encoder_out, enc_self_attns = self.encoder(enc_inputs)\n",
        "        print(\"Transformer Encoder encoder_out shape: \", encoder_out.shape)\n",
        "        # encoder_out: [batch_size, 196, 2048]\n",
        "        print(\"** Running Transformer Decoder\")\n",
        "        predictions, encoded_captions, decode_lengths, sort_ind, dec_self_attns, dec_enc_attns = self.decoder(encoder_out, encoded_captions, caption_lengths)\n",
        "        alphas = {\"enc_self_attns\": enc_self_attns, \"dec_self_attns\": dec_self_attns, \"dec_enc_attns\": dec_enc_attns}\n",
        "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind"
      ],
      "metadata": {
        "id": "8ft49co9cdEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN_Encoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoded_image_size=14, attention_method=\"ByPixel\"):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        self.enc_image_size = encoded_image_size\n",
        "        self.attention_method = attention_method\n",
        "\n",
        "        resnet = torchvision.models.resnet101(pretrained=True)  # pretrained ImageNet ResNet-101\n",
        "\n",
        "        # Remove linear and pool layers (since we're not doing classification)\n",
        "        # Specifically, Remove: AdaptiveAvgPool2d(output_size=(1, 1)), Linear(in_features=2048, out_features=1000, bias=True)]\n",
        "        modules = list(resnet.children())[:-2]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "\n",
        "        if self.attention_method == \"ByChannel\":\n",
        "            self.cnn1 = nn.Conv2d(in_channels=2048, out_channels=512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
        "            self.bn1 = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "            self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Resize image to fixed size to allow input images of variable size\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
        "\n",
        "        self.fine_tune()\n",
        "\n",
        "    def forward(self, images):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param images: images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n",
        "        :return: encoded images [batch_size, encoded_image_size=14, encoded_image_size=14, 2048]\n",
        "        \"\"\"\n",
        "        out = self.resnet(images)  # (batch_size, 2048, image_size/32, image_size/32)\n",
        "        if self.attention_method == \"ByChannel\":  # [batch_size, 2048, 8, 8] -> # [batch_size, 512, 8, 8]\n",
        "            out = self.relu(self.bn1(self.cnn1(out)))\n",
        "        out = self.adaptive_pool(out)  # [batch_size, 2048/512, 8, 8] -> [batch_size, 2048/512, 14, 14]\n",
        "        out = out.permute(0, 2, 3, 1)\n",
        "        return out\n",
        "\n",
        "    def fine_tune(self, fine_tune=True):\n",
        "        \"\"\"\n",
        "        Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n",
        "        :param fine_tune: Allow?\n",
        "        \"\"\"\n",
        "        for p in self.resnet.parameters():\n",
        "            p.requires_grad = False\n",
        "        # If fine-tuning, only fine-tune convolutional blocks 2 through 4\n",
        "        for c in list(self.resnet.children())[5:]:\n",
        "            for p in c.parameters():\n",
        "                p.requires_grad = fine_tune\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention Network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
        "        \"\"\"\n",
        "        :param encoder_dim: feature size of encoded images\n",
        "        :param decoder_dim: size of decoder's RNN\n",
        "        :param attention_dim: size of the attention network\n",
        "        \"\"\"\n",
        "        super(Attention, self).__init__()\n",
        "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n",
        "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n",
        "        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n",
        "\n",
        "    def forward(self, encoder_out, decoder_hidden):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
        "        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n",
        "        :return: attention weighted encoding, weights\n",
        "        \"\"\"\n",
        "        att1 = self.encoder_att(encoder_out)  # [batch_size_t, num_pixels=196, 2048] -> [batch_size_t, num_pixels, attention_dim]\n",
        "        att2 = self.decoder_att(decoder_hidden)  # [batch_size_t, decoder_dim=512] -> [batch_size_t, attention_dim]\n",
        "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # [batch_size_t, num_pixels=196, attention_dim] -> [batch_size_t, num_pixels]\n",
        "        alpha = self.softmax(att)  # [batch_size_t, num_pixels=196]\n",
        "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # [batch_size_t, encoder_dim=2048]\n",
        "\n",
        "        return attention_weighted_encoding, alpha\n",
        "\n",
        "\n",
        "class DecoderWithAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n",
        "        \"\"\"\n",
        "        :param attention_dim: size of attention network\n",
        "        :param embed_dim: embedding size\n",
        "        :param decoder_dim: size of decoder's RNN\n",
        "        :param vocab_size: size of vocabulary\n",
        "        :param encoder_dim: feature size of encoded images\n",
        "        :param dropout: dropout\n",
        "        \"\"\"\n",
        "        super(DecoderWithAttention, self).__init__()\n",
        "\n",
        "        self.encoder_dim = encoder_dim\n",
        "        self.attention_dim = attention_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)  # embedding layer\n",
        "        self.dropout = nn.Dropout(p=self.dropout)\n",
        "        self.lstm = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n",
        "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n",
        "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n",
        "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n",
        "        self.init_weights()  # initialize some layers with the uniform distribution\n",
        "        self.fine_tune_embeddings()\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        Initializes some parameters with values from the uniform distribution, for easier convergence.\n",
        "        \"\"\"\n",
        "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    def load_pretrained_embeddings(self, embeddings):\n",
        "        \"\"\"\n",
        "        Loads embedding layer with pre-trained embeddings.\n",
        "        :param embeddings: pre-trained embeddings\n",
        "        \"\"\"\n",
        "        self.embedding.weight = nn.Parameter(embeddings)\n",
        "\n",
        "    def fine_tune_embeddings(self, fine_tune=True):\n",
        "        \"\"\"\n",
        "        Allow fine-tuning of embedding layer? (Only makes sense to not-allow if using pre-trained embeddings).\n",
        "        :param fine_tune: Allow?\n",
        "        \"\"\"\n",
        "        for p in self.embedding.parameters():\n",
        "            p.requires_grad = fine_tune\n",
        "\n",
        "    def init_hidden_state(self, encoder_out):\n",
        "        \"\"\"\n",
        "        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
        "        :return: hidden state, cell state\n",
        "        \"\"\"\n",
        "        mean_encoder_out = encoder_out.mean(dim=1)  # [batch_size, 196, 2048] -> [batch_size, 2048]\n",
        "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
        "        c = self.init_c(mean_encoder_out)\n",
        "        return h, c\n",
        "\n",
        "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n",
        "        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n",
        "        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n",
        "        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n",
        "        \"\"\"\n",
        "        # [batch_size, 14, 14, 2048]/[batch_size, 196, 2048] -> [batch_size, 196, 2048]\n",
        "        batch_size = encoder_out.size(0)\n",
        "        encoder_dim = encoder_out.size(-1)\n",
        "        vocab_size = self.vocab_size\n",
        "\n",
        "        # Flatten image -> [batch_size, num_pixels=196, encoder_dim=2048]\n",
        "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)\n",
        "        num_pixels = encoder_out.size(1)\n",
        "\n",
        "        # Sort input data by decreasing lengths; why? For each of data in the batch, when len(prediction) = len(caption_lengths), Stop.\n",
        "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
        "        encoder_out = encoder_out[sort_ind]\n",
        "        encoded_captions = encoded_captions[sort_ind]\n",
        "\n",
        "        # Embedding\n",
        "        embeddings = self.embedding(encoded_captions)  # [batch_size, max_caption_length=52, embed_dim]\n",
        "\n",
        "        # Initialize LSTM state\n",
        "        h, c = self.init_hidden_state(encoder_out)  # [batch_size, decoder_dim]\n",
        "\n",
        "        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
        "        # So, decoding lengths are actual lengths - 1\n",
        "        decode_lengths = (caption_lengths - 1).tolist()\n",
        "\n",
        "        # Create tensors to hold word predicion scores and alphas\n",
        "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n",
        "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n",
        "\n",
        "        # At each time-step, decode by\n",
        "        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n",
        "        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n",
        "        for t in range(max(decode_lengths)):\n",
        "            batch_size_t = sum([l > t for l in decode_lengths])\n",
        "            # alpha: [batch_size_t, 196]\n",
        "            # attention_weighted_encoding: [batch_size_t, 2048]\n",
        "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t], h[:batch_size_t])\n",
        "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n",
        "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
        "            # torch.cat([batch_size_t, 500], [batch_size_t, 2048], dim=1) = [batch_size_t, 2548] -> [batch_size_t, 512]\n",
        "            h, c = self.lstm(\n",
        "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
        "                (h[:batch_size_t], c[:batch_size_t]))\n",
        "            preds = self.fc(self.dropout(h))  # [batch_size_t, vocab_size]\n",
        "            predictions[:batch_size_t, t, :] = preds\n",
        "            alphas[:batch_size_t, t, :] = alpha\n",
        "\n",
        "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind"
      ],
      "metadata": {
        "id": "LfXKqRlnDEWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean, std = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n",
        "img_size = 256 #MSCOCO image size of 256x256\n",
        "\n",
        "train_transform = transforms.Compose([ \n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.RandomHorizontalFlip(), \n",
        "    transforms.ToTensor(), \n",
        "    transforms.Normalize(mean, std)])\n",
        "\n",
        "train_loader, train_dataset = getLoader(root_dir=train_image_dir, \n",
        "                                         caption_path=train_caption_path, \n",
        "                                         word_frequency=5,\n",
        "                                         transform=train_transform, \n",
        "                                         batch_size=32, \n",
        "                                         num_workers=0, \n",
        "                                         shuffle=True, \n",
        "                                         pin_memory=True)\n"
      ],
      "metadata": {
        "id": "39lkanl3cjBv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb0178d4-c6db-47e1-ae40-b67af94d66ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imgs, captions, caplens = next(iter(train_loader))\n",
        "print(imgs.shape)\n",
        "print(captions.shape)\n",
        "print(caplens.shape)"
      ],
      "metadata": {
        "id": "4GtLCxw3G_2U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a69925a0-5574-48a1-d467-786f31502273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 256, 256])\n",
            "torch.Size([32, 52])\n",
            "torch.Size([32, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_lr = 1e-4\n",
        "decoder_lr = 1e-6 #learning rate for decoder\n",
        "fine_tune_encoder = False\n",
        "emb_dim = 300 #dimension of word embeddings\n",
        "encoder_layers = 2 #the number of layers of encoder in Transformer\n",
        "decoder_layers = 6 #the number of layers of decoder in Transformer\n",
        "dropout = 0.1 #dropout\n",
        "attention_method = \"ByPixel\" #which attention method to use?\n",
        "n_heads = 8 #Multi-head attention\n",
        "embedding_path = os.path.join(\"./data\", \"glove.6B.300d.txt\")\n",
        "fine_tune_embedding = False\n",
        "word_map = train_dataset.vocab.stoi\n",
        "print(\"Number of words in word map: \", len(word_map))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8pg8dUUIB2D",
        "outputId": "6ae4c339-b22e-4002-8528-e985eec8e2c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words in word map:  2544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = CNN_Encoder()\n",
        "encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
        "                                     lr=encoder_lr) if fine_tune_encoder else None\n",
        "decoder = Transformer(vocab_size=len(word_map), embed_dim=emb_dim, encoder_layers=encoder_layers,\n",
        "                                  decoder_layers=decoder_layers, dropout=dropout,\n",
        "                                  attention_method=attention_method, n_heads=n_heads)"
      ],
      "metadata": {
        "id": "KqwPAFC6PfGG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74d8caf1-303f-46c7-ef1d-6cd3888b4f00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()), lr=decoder_lr)"
      ],
      "metadata": {
        "id": "fG2QPIgKLjYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import codecs\n",
        "all_word_embeds = {}\n",
        "for i, line in enumerate(codecs.open(embedding_path, 'r', 'utf-8')):\n",
        "    s = line.strip().split()\n",
        "    all_word_embeds[s[0]] = np.array([float(i) for i in s[1:]])"
      ],
      "metadata": {
        "id": "EhwO8zkDLjak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_word_embeds.keys())\n",
        "emb_dim = list(all_word_embeds.values())[-1].size\n",
        "print(emb_dim) #Each word is descrited in 300 dimensions\n",
        "word_embeds = np.random.uniform(-np.sqrt(0.06), np.sqrt(0.06), (len(word_map), emb_dim))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjX7W8pqPcCF",
        "outputId": "3c122382-fa02-4809-a748-5ba930046fe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for w in word_map:\n",
        "    if w in all_word_embeds:\n",
        "        word_embeds[word_map[w]] = all_word_embeds[w]\n",
        "    elif w.lower() in all_word_embeds:\n",
        "        word_embeds[word_map[w]] = all_word_embeds[w.lower()]\n",
        "    else:\n",
        "        # <pad> <start> <end> <unk>\n",
        "        embedding_i = torch.ones(1, emb_dim)\n",
        "        torch.nn.init.xavier_uniform_(embedding_i)\n",
        "        word_embeds[word_map[w]] = embedding_i\n",
        "\n",
        "word_embeds = torch.FloatTensor(word_embeds)\n",
        "decoder.load_pretrained_embeddings(word_embeds)\n",
        "decoder.fine_tune_embeddings(fine_tune_embedding)\n",
        "print('Loaded {} pre-trained word embeddings.'.format(len(word_embeds)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvS4DSgAQ3rd",
        "outputId": "d3c24e0a-101c-4350-fafd-2e0a884f18db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 2544 pre-trained word embeddings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "7j2syoEoRQpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"imgs shape before encoder (CNN): \", imgs.shape)\n",
        "enc_imgs = encoder(imgs)\n",
        "print(\"imgs shape after encoder (CNN): \", enc_imgs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfbIuc14SZrP",
        "outputId": "1b6c6a8f-b14b-4a10-fd0d-aa3ce8061976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "imgs shape before encoder (CNN):  torch.Size([32, 3, 256, 256])\n",
            "imgs shape after encoder (CNN):  torch.Size([32, 14, 14, 2048])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(enc_imgs, captions, caplens)"
      ],
      "metadata": {
        "id": "R1lex9NNTE-w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "224edba0-92e1-4aad-baa8-f9f35b19105b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** Running Transformer Encoder\n",
            "Q: torch.Size([32, 196, 2048]), K: torch.Size([32, 196, 2048]), V: torch.Size([32, 196, 2048])\n",
            "q_s: torch.Size([32, 8, 196, 64]), k_s: torch.Size([32, 8, 196, 64]), v_s: torch.Size([32, 8, 196, 64])\n",
            "After output + residual in MHA:  torch.Size([32, 196, 2048])\n",
            "Q: torch.Size([32, 196, 2048]), K: torch.Size([32, 196, 2048]), V: torch.Size([32, 196, 2048])\n",
            "q_s: torch.Size([32, 8, 196, 64]), k_s: torch.Size([32, 8, 196, 64]), v_s: torch.Size([32, 8, 196, 64])\n",
            "After output + residual in MHA:  torch.Size([32, 196, 2048])\n",
            "Transformer Encoder encoder_out shape:  torch.Size([32, 196, 2048])\n",
            "** Running Transformer Decoder\n",
            "caption lengths in decoder : tensor([20, 18, 18, 17, 16, 15, 15, 15, 14, 14, 13, 13, 13, 12, 12, 12, 12, 11,\n",
            "        11, 11, 11, 11, 10, 10, 10,  9,  9,  9,  8,  8,  7,  6])\n",
            "decode  lengths in decoder : [19, 17, 17, 16, 15, 14, 14, 14, 13, 13, 12, 12, 12, 11, 11, 11, 11, 10, 10, 10, 10, 10, 9, 9, 9, 8, 8, 8, 7, 7, 6, 5]\n",
            "Output of Target Embedding:  torch.Size([32, 52, 300])\n",
            "Output after Positional Encoding:  torch.Size([32, 52, 300])\n",
            "Output after Target Embedding and Positional Encoding:  torch.Size([32, 52, 300])\n",
            "Q: torch.Size([32, 52, 300]), K: torch.Size([32, 52, 300]), V: torch.Size([32, 52, 300])\n",
            "q_s: torch.Size([32, 8, 52, 64]), k_s: torch.Size([32, 8, 52, 64]), v_s: torch.Size([32, 8, 52, 64])\n",
            "After output + residual in MHA:  torch.Size([32, 52, 300])\n",
            "Decoder: Output of Self Attention:  torch.Size([32, 52, 300])\n",
            "Q: torch.Size([32, 52, 300]), K: torch.Size([32, 196, 2048]), V: torch.Size([32, 196, 2048])\n",
            "q_s: torch.Size([32, 8, 52, 64]), k_s: torch.Size([32, 8, 196, 64]), v_s: torch.Size([32, 8, 196, 64])\n",
            "After output + residual in MHA:  torch.Size([32, 52, 300])\n",
            "Decoder: Output of Encoder-Decoder Attention:  torch.Size([32, 52, 300])\n",
            "Decoder: Output of FFN:  torch.Size([32, 52, 300])\n",
            "Q: torch.Size([32, 52, 300]), K: torch.Size([32, 52, 300]), V: torch.Size([32, 52, 300])\n",
            "q_s: torch.Size([32, 8, 52, 64]), k_s: torch.Size([32, 8, 52, 64]), v_s: torch.Size([32, 8, 52, 64])\n",
            "After output + residual in MHA:  torch.Size([32, 52, 300])\n",
            "Decoder: Output of Self Attention:  torch.Size([32, 52, 300])\n",
            "Q: torch.Size([32, 52, 300]), K: torch.Size([32, 196, 2048]), V: torch.Size([32, 196, 2048])\n",
            "q_s: torch.Size([32, 8, 52, 64]), k_s: torch.Size([32, 8, 196, 64]), v_s: torch.Size([32, 8, 196, 64])\n",
            "After output + residual in MHA:  torch.Size([32, 52, 300])\n",
            "Decoder: Output of Encoder-Decoder Attention:  torch.Size([32, 52, 300])\n",
            "Decoder: Output of FFN:  torch.Size([32, 52, 300])\n",
            "Q: torch.Size([32, 52, 300]), K: torch.Size([32, 52, 300]), V: torch.Size([32, 52, 300])\n",
            "q_s: torch.Size([32, 8, 52, 64]), k_s: torch.Size([32, 8, 52, 64]), v_s: torch.Size([32, 8, 52, 64])\n",
            "After output + residual in MHA:  torch.Size([32, 52, 300])\n",
            "Decoder: Output of Self Attention:  torch.Size([32, 52, 300])\n",
            "Q: torch.Size([32, 52, 300]), K: torch.Size([32, 196, 2048]), V: torch.Size([32, 196, 2048])\n",
            "q_s: torch.Size([32, 8, 52, 64]), k_s: torch.Size([32, 8, 196, 64]), v_s: torch.Size([32, 8, 196, 64])\n",
            "After output + residual in MHA:  torch.Size([32, 52, 300])\n",
            "Decoder: Output of Encoder-Decoder Attention:  torch.Size([32, 52, 300])\n",
            "Decoder: Output of FFN:  torch.Size([32, 52, 300])\n",
            "Q: torch.Size([32, 52, 300]), K: torch.Size([32, 52, 300]), V: torch.Size([32, 52, 300])\n",
            "q_s: torch.Size([32, 8, 52, 64]), k_s: torch.Size([32, 8, 52, 64]), v_s: torch.Size([32, 8, 52, 64])\n",
            "After output + residual in MHA:  torch.Size([32, 52, 300])\n",
            "Decoder: Output of Self Attention:  torch.Size([32, 52, 300])\n",
            "Q: torch.Size([32, 52, 300]), K: torch.Size([32, 196, 2048]), V: torch.Size([32, 196, 2048])\n",
            "q_s: torch.Size([32, 8, 52, 64]), k_s: torch.Size([32, 8, 196, 64]), v_s: torch.Size([32, 8, 196, 64])\n",
            "After output + residual in MHA:  torch.Size([32, 52, 300])\n",
            "Decoder: Output of Encoder-Decoder Attention:  torch.Size([32, 52, 300])\n",
            "Decoder: Output of FFN:  torch.Size([32, 52, 300])\n",
            "Q: torch.Size([32, 52, 300]), K: torch.Size([32, 52, 300]), V: torch.Size([32, 52, 300])\n",
            "q_s: torch.Size([32, 8, 52, 64]), k_s: torch.Size([32, 8, 52, 64]), v_s: torch.Size([32, 8, 52, 64])\n",
            "After output + residual in MHA:  torch.Size([32, 52, 300])\n",
            "Decoder: Output of Self Attention:  torch.Size([32, 52, 300])\n",
            "Q: torch.Size([32, 52, 300]), K: torch.Size([32, 196, 2048]), V: torch.Size([32, 196, 2048])\n",
            "q_s: torch.Size([32, 8, 52, 64]), k_s: torch.Size([32, 8, 196, 64]), v_s: torch.Size([32, 8, 196, 64])\n",
            "After output + residual in MHA:  torch.Size([32, 52, 300])\n",
            "Decoder: Output of Encoder-Decoder Attention:  torch.Size([32, 52, 300])\n",
            "Decoder: Output of FFN:  torch.Size([32, 52, 300])\n",
            "Q: torch.Size([32, 52, 300]), K: torch.Size([32, 52, 300]), V: torch.Size([32, 52, 300])\n",
            "q_s: torch.Size([32, 8, 52, 64]), k_s: torch.Size([32, 8, 52, 64]), v_s: torch.Size([32, 8, 52, 64])\n",
            "After output + residual in MHA:  torch.Size([32, 52, 300])\n",
            "Decoder: Output of Self Attention:  torch.Size([32, 52, 300])\n",
            "Q: torch.Size([32, 52, 300]), K: torch.Size([32, 196, 2048]), V: torch.Size([32, 196, 2048])\n",
            "q_s: torch.Size([32, 8, 52, 64]), k_s: torch.Size([32, 8, 196, 64]), v_s: torch.Size([32, 8, 196, 64])\n",
            "After output + residual in MHA:  torch.Size([32, 52, 300])\n",
            "Decoder: Output of Encoder-Decoder Attention:  torch.Size([32, 52, 300])\n",
            "Decoder: Output of FFN:  torch.Size([32, 52, 300])\n",
            "Output after Decode Layers:  torch.Size([32, 52, 300])\n",
            "Output after Projection Layers:  torch.Size([32, 52, 2544])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Scores shape: \", scores.shape)\n",
        "print(\"Scores shape: \", caps_sorted.shape)\n",
        "print(\"decode_lengths  : \", decode_lengths)\n",
        "print(\"original caplens: \", caplens.squeeze(1).tolist())\n",
        "print(\"alphas keys: \", alphas.keys())\n",
        "print(\"Scores shape: \", sort_ind.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwObJcQCNlro",
        "outputId": "d5711eaf-fc95-4f92-9e94-fdfce287f4ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scores shape:  torch.Size([32, 52, 2544])\n",
            "Scores shape:  torch.Size([32, 52])\n",
            "decode_lengths  :  [19, 17, 17, 16, 15, 14, 14, 14, 13, 13, 12, 12, 12, 11, 11, 11, 11, 10, 10, 10, 10, 10, 9, 9, 9, 8, 8, 8, 7, 7, 6, 5]\n",
            "original caplens:  [10, 16, 11, 14, 8, 10, 18, 15, 11, 8, 9, 12, 9, 11, 11, 12, 6, 13, 14, 12, 10, 18, 13, 20, 15, 15, 17, 11, 13, 12, 9, 7]\n",
            "alphas keys:  dict_keys(['enc_self_attns', 'dec_self_attns', 'dec_enc_attns'])\n",
            "Scores shape:  torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
        "targets = caps_sorted[:, 1:]"
      ],
      "metadata": {
        "id": "YTfsjeS-re3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = pack_padded_sequence(scores, decode_lengths, batch_first=True).data\n",
        "targets = pack_padded_sequence(targets, decode_lengths, batch_first=True).data"
      ],
      "metadata": {
        "id": "cy28sW9AASAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(scores.shape)\n",
        "print(targets.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9lq0ZborsnZ",
        "outputId": "fae77394-a922-4917-a60c-09cb32955421"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([358, 2544])\n",
            "torch.Size([358])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = criterion(scores, targets)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8VvqjYqrxoQ",
        "outputId": "ce7c7181-a062-4130-fa9f-3340895ba801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(8.0074, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Inference"
      ],
      "metadata": {
        "id": "5rWQZodP18MG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zjFmq1l8HZQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6EfT66UyHK8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Custom_Transformer(img_size = img_size,\n",
        "                           patch_size = patch_size,\n",
        "                           in_chans = in_chans,\n",
        "                           embed_dim = embed_dim,\n",
        "                           vocab_size = vocab_size,\n",
        "                           encoder_depth = encoder_depth,\n",
        "                           decoder_depth = decoder_depth,\n",
        "                           encoder_heads = encoder_heads,\n",
        "                           decoder_heads = decoder_heads,\n",
        "                           mlp_ratio = mlp_ratio,\n",
        "                           pos_p = pos_p,\n",
        "                           attn_p = attn_p,\n",
        "                           ffn_p = ffn_p)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "total_params = sum(\n",
        "\tparam.numel() for param in model.parameters()\n",
        ")\n",
        "print(f\"Current Model has {total_params} parameters\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "print(f\"Running Model on {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH9s2FPDsk1q",
        "outputId": "1a32d2a0-1837-4e2d-c66d-20fb16b646c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Model has 179963813 parameters\n",
            "Running Model on cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_model = True\n",
        "saved_model_path = os.path.join(models_path, \"my_checkpoint.pth.tar\")\n",
        "step = 0\n",
        "loss_dict = dict()\n",
        "loss_dict['train'] = list()\n",
        "loss_dict['val'] = list()\n",
        "loss_path = os.path.join(models_path, \"image_captioning_loss.pkl\")\n",
        "\n",
        "if load_model:\n",
        "    model, optimizer, step = load_checkpoint(torch.load(saved_model_path), model, optimizer)"
      ],
      "metadata": {
        "id": "Ta1J9fbG0Y7Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b6f603d-843e-4f5c-aeb3-19789fb5f796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading checkpoint!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98iTIgcipJ0O",
        "outputId": "cf174b6b-dfa5-4ab7-b0d6-8688ce0edd04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Custom_Transformer(\n",
              "  (encoder): Custom_TransformerEncoder(\n",
              "    (patch_embed): PatchEmbed(\n",
              "      (patching): Conv2d(3, 256, kernel_size=(16, 16), stride=(16, 16))\n",
              "    )\n",
              "    (pos_encoder): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (blocks): ModuleList(\n",
              "      (0): EncoderBlock(\n",
              "        (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (ffn): FFN(\n",
              "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): EncoderBlock(\n",
              "        (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (ffn): FFN(\n",
              "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): EncoderBlock(\n",
              "        (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (ffn): FFN(\n",
              "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): EncoderBlock(\n",
              "        (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (ffn): FFN(\n",
              "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): EncoderBlock(\n",
              "        (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (ffn): FFN(\n",
              "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): EncoderBlock(\n",
              "        (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (ffn): FFN(\n",
              "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): EncoderBlock(\n",
              "        (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (ffn): FFN(\n",
              "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): EncoderBlock(\n",
              "        (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (ffn): FFN(\n",
              "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): Custom_TransformerDecoder(\n",
              "    (word_embed): Embedding(336037, 256)\n",
              "    (pos_decoder): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (blocks): ModuleList(\n",
              "      (0): DecoderBlock(\n",
              "        (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "        (mask_attn): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (cross_attn): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (ffn): FFN(\n",
              "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): DecoderBlock(\n",
              "        (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "        (mask_attn): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (cross_attn): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (ffn): FFN(\n",
              "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): DecoderBlock(\n",
              "        (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "        (mask_attn): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (cross_attn): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (ffn): FFN(\n",
              "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): DecoderBlock(\n",
              "        (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "        (mask_attn): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (cross_attn): MultiHeadAttention(\n",
              "          (q_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k_linear): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (out): Linear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (ffn): FFN(\n",
              "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (final_layer): Linear(in_features=256, out_features=336037, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imgs, captions = next(iter(test_loader))"
      ],
      "metadata": {
        "id": "16NTmtYgpK7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(imgs.shape)\n",
        "print(captions.shape)\n",
        "\n",
        "x = imgs[0]\n",
        "y = captions[0]\n",
        "plt.imshow(x.permute(1,2,0))\n",
        "print(y)\n",
        "for i in y:\n",
        "    print(test_dataset.vocab.itos[int(i)],end=\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "UAl25Nrcypvi",
        "outputId": "a73b3c66-7ef3-40d3-92f1-49bd27fccd95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 256, 256])\n",
            "torch.Size([32, 21])\n",
            "tensor([    1, 57183, 49641, 57582, 57338, 57617,     3, 57428, 57617,     3,\n",
            "        50310, 57487, 55807, 57307, 53396, 57621,     2,     0,     0,     0,\n",
            "            0])\n",
            "<SOS> an elderly man wearing a <UNK> riding a <UNK> scooter while shopping for shoes . <EOS> <PAD> <PAD> <PAD> <PAD> "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imgs = imgs.to(device)\n",
        "features = model.encoder(imgs)"
      ],
      "metadata": {
        "id": "sVXatUdKy3sF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.final_layer(features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "p1Mpge7M0BtW",
        "outputId": "375ca395-7cf0-484b-ab25-6efadfc78525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-bfc70b963bdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 10.26 GiB (GPU 0; 15.90 GiB total capacity; 9.12 GiB already allocated; 5.70 GiB free; 9.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "len_q = 196\n",
        "len_k = 196\n",
        "max_len = 52\n",
        "embed_dim = 512\n",
        "encoder_dim = 2048\n",
        "QKVdim = 64\n",
        "Q_dim=embed_dim\n",
        "K_dim=encoder_dim\n",
        "n_heads=8\n",
        "\n",
        "Q = torch.ones([batch_size, max_len, embed_dim])\n",
        "K = torch.ones([batch_size, len_k, encoder_dim])\n",
        "V = torch.ones([batch_size, len_k, encoder_dim])"
      ],
      "metadata": {
        "id": "ViNsb1c80qI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W_Q = nn.Linear(Q_dim, QKVdim * n_heads)\n",
        "W_K = nn.Linear(K_dim, QKVdim * n_heads)\n",
        "W_V = nn.Linear(K_dim, QKVdim * n_heads)\n",
        "W_O = nn.Linear(n_heads * QKVdim, embed_dim)"
      ],
      "metadata": {
        "id": "OLMUdWJo6oGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W_O"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EG4mMltC_T-J",
        "outputId": "f8389749-a49c-4924-f522-ddb5b98659bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=512, out_features=512, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_s = W_Q(Q).view(batch_size, -1, n_heads, QKVdim).transpose(1, 2)\n",
        "k_s = W_K(K).view(batch_size, -1, n_heads, QKVdim).transpose(1, 2)\n",
        "v_s = W_V(V).view(batch_size, -1, n_heads, QKVdim).transpose(1, 2)"
      ],
      "metadata": {
        "id": "tRs-aGx-6pYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(q_s.shape)\n",
        "print(k_s.shape)\n",
        "print(v_s.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGfFtDCh8Y9k",
        "outputId": "d98997f6-72d6-46f7-8af6-1b900ca4cc57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 8, 52, 64])\n",
            "torch.Size([10, 8, 196, 64])\n",
            "torch.Size([10, 8, 196, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = torch.matmul(q_s, k_s.transpose(-1, -2)) / np.sqrt(QKVdim)\n",
        "print(\"Scores shape: \", scores.shape)\n",
        "attn = nn.Softmax(dim=-1)(scores)\n",
        "print(\"Attention shape: \", attn.shape)\n",
        "context = torch.matmul(attn, v_s)\n",
        "print(\"Context shape: \", context.shape) #[batch_size, n_heads, len_q, QKVdim]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8M7e7ZLa8ghJ",
        "outputId": "7a5dc1a7-8a0d-4cb8-909e-91d27408a140"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scores shape:  torch.Size([10, 8, 52, 196])\n",
            "Attention shape:  torch.Size([10, 8, 52, 196])\n",
            "Context shape:  torch.Size([10, 8, 52, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * QKVdim) # output: [batch_size, len_q, embed_dim]\n",
        "print(\"Context shape for final linear layer: \", context.shape) #[batch_size, n_heads, len_q, QKVdim]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REOoHYeK8oek",
        "outputId": "bedab927-a1d8-4cbc-be8b-bc34ee624a32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context shape for final linear layer:  torch.Size([10, 52, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = W_O(context)\n",
        "print(\"Output shape: \", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBI8zEEf-dZY",
        "outputId": "ebe95226-dc4b-4dd8-c1e5-6048765de037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape:  torch.Size([10, 52, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean, std = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n",
        "img_size = 256\n",
        "\n",
        "train_transform = transforms.Compose([ \n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.RandomHorizontalFlip(), \n",
        "    transforms.ToTensor(), \n",
        "    transforms.Normalize(mean, std)])\n",
        "\n",
        "train_loader, train_dataset = getLoader(root_dir=train_image_dir, \n",
        "                                         caption_path=train_caption_path, \n",
        "                                         word_frequency=5,\n",
        "                                         transform=train_transform, \n",
        "                                         batch_size=10, \n",
        "                                         num_workers=0, \n",
        "                                         shuffle=True, \n",
        "                                         pin_memory=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "Rm23GD_lBxqO",
        "outputId": "caf19048-09f0-40d2-de09-41f6b0c8f379"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-87078342671b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     transforms.Normalize(mean, std)])\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m train_loader, train_dataset = getLoader(root_dir=train_image_dir, \n\u001b[0m\u001b[1;32m     11\u001b[0m                                          \u001b[0mcaption_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_caption_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                          \u001b[0mword_frequency\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'getLoader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "igHAw9_cBEjJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Ry6Pk1kDA7LH",
        "HANXhSr_raT4",
        "VXA9naVy3GXK",
        "ZLX7JW2G3KNt"
      ],
      "name": "ImgCaptioning_Transformer_flickr8k",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyODtymXLFjxaPTbPfvSXTQ4",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}